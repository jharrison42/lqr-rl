{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import yaml\n",
    "from klqr import *\n",
    "from train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yml','r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: (?, 3)\n",
      "********** Iteration 1 ************\n",
      "\tLast Episode Reward: -122768\n",
      "********** Iteration 2 ************\n",
      "\tLast Episode Reward: -251250\n",
      "********** Iteration 3 ************\n",
      "\tLast Episode Reward: -10421\n",
      "********** Iteration 4 ************\n",
      "\tLast Episode Reward: -13132\n",
      "********** Iteration 5 ************\n",
      "[[ 4.7175884   2.5355196  -0.80097485]\n",
      " [ 2.5355196   2.5242977  -0.8128734 ]\n",
      " [-0.80097485 -0.8128734   1.1462948 ]]\n",
      "\tLast Episode Reward: -898\n",
      "********** Iteration 6 ************\n",
      "\tLast Episode Reward: -1962427\n",
      "********** Iteration 7 ************\n",
      "\tLast Episode Reward: -2066711\n",
      "********** Iteration 8 ************\n",
      "\tLast Episode Reward: -2024696\n",
      "********** Iteration 9 ************\n",
      "\tLast Episode Reward: -1553755\n",
      "********** Iteration 10 ************\n",
      "[[ 2.2230268   0.40118304 -1.3500311 ]\n",
      " [ 0.40118304  1.339439   -0.7751753 ]\n",
      " [-1.3500311  -0.7751753   1.9038198 ]]\n",
      "\tLast Episode Reward: -157800\n",
      "********** Iteration 11 ************\n",
      "\tLast Episode Reward: -2232073\n",
      "********** Iteration 12 ************\n",
      "\tLast Episode Reward: -135176\n",
      "********** Iteration 13 ************\n",
      "\tLast Episode Reward: -125268\n",
      "********** Iteration 14 ************\n",
      "\tLast Episode Reward: -20476\n",
      "********** Iteration 15 ************\n",
      "[[ 2.8990269   0.77269626 -2.446459  ]\n",
      " [ 0.77269626  1.9611115  -0.3690896 ]\n",
      " [-2.446459   -0.3690896   3.014855  ]]\n",
      "\tLast Episode Reward: -163865\n",
      "********** Iteration 16 ************\n",
      "\tLast Episode Reward: -143439\n",
      "********** Iteration 17 ************\n",
      "\tLast Episode Reward: -102849\n",
      "********** Iteration 18 ************\n",
      "\tLast Episode Reward: -112544\n",
      "********** Iteration 19 ************\n",
      "\tLast Episode Reward: -1221\n",
      "********** Iteration 20 ************\n",
      "[[ 4.538662   1.6290894 -5.288267 ]\n",
      " [ 1.6290894  2.3659654 -1.7306495]\n",
      " [-5.288267  -1.7306495  7.486438 ]]\n",
      "\tLast Episode Reward: -1731\n",
      "********** Iteration 21 ************\n",
      "\tLast Episode Reward: -132216\n",
      "********** Iteration 22 ************\n",
      "\tLast Episode Reward: -118958\n",
      "********** Iteration 23 ************\n",
      "\tLast Episode Reward: -68646\n",
      "********** Iteration 24 ************\n",
      "\tLast Episode Reward: -204800\n",
      "********** Iteration 25 ************\n",
      "[[  7.6445785   3.0292976 -11.811824 ]\n",
      " [  3.0292976   2.908534   -4.6716857]\n",
      " [-11.811824   -4.6716857  20.189354 ]]\n",
      "\tLast Episode Reward: -275100\n",
      "********** Iteration 26 ************\n",
      "\tLast Episode Reward: -140417\n",
      "********** Iteration 27 ************\n",
      "\tLast Episode Reward: -170930\n",
      "********** Iteration 28 ************\n",
      "\tLast Episode Reward: -212825\n",
      "********** Iteration 29 ************\n",
      "\tLast Episode Reward: -372440\n",
      "********** Iteration 30 ************\n",
      "[[ 10.398141    3.378613  -18.135063 ]\n",
      " [  3.378613    2.5970566  -5.828949 ]\n",
      " [-18.135063   -5.828949   34.11891  ]]\n",
      "\tLast Episode Reward: -309261\n",
      "********** Iteration 31 ************\n",
      "\tLast Episode Reward: -806263\n",
      "********** Iteration 32 ************\n",
      "\tLast Episode Reward: -520575\n",
      "********** Iteration 33 ************\n",
      "\tLast Episode Reward: -343992\n",
      "********** Iteration 34 ************\n",
      "\tLast Episode Reward: -103676\n",
      "********** Iteration 35 ************\n",
      "[[  8.098576    1.1291676 -13.74349  ]\n",
      " [  1.1291676   1.4449961  -1.5728896]\n",
      " [-13.74349    -1.5728896  26.175293 ]]\n",
      "\tLast Episode Reward: -727\n",
      "********** Iteration 36 ************\n",
      "\tLast Episode Reward: -358607\n",
      "********** Iteration 37 ************\n",
      "\tLast Episode Reward: -182577\n",
      "********** Iteration 38 ************\n",
      "\tLast Episode Reward: -343\n",
      "********** Iteration 39 ************\n",
      "\tLast Episode Reward: -130\n",
      "********** Iteration 40 ************\n",
      "[[ 10.756079   -0.6443558 -17.940704 ]\n",
      " [ -0.6443558   1.2995453   1.6899645]\n",
      " [-17.940704    1.6899645  33.564423 ]]\n",
      "\tLast Episode Reward: -156\n",
      "********** Iteration 41 ************\n",
      "\tLast Episode Reward: -6860\n",
      "********** Iteration 42 ************\n",
      "\tLast Episode Reward: -173\n",
      "********** Iteration 43 ************\n",
      "\tLast Episode Reward: -117\n",
      "********** Iteration 44 ************\n",
      "\tLast Episode Reward: -82\n",
      "********** Iteration 45 ************\n",
      "[[  8.699333   -2.131524  -13.805107 ]\n",
      " [ -2.131524    1.8902428   4.3750534]\n",
      " [-13.805107    4.3750534  26.383486 ]]\n",
      "\tLast Episode Reward: -138\n",
      "********** Iteration 46 ************\n",
      "\tLast Episode Reward: -117\n",
      "********** Iteration 47 ************\n",
      "\tLast Episode Reward: -76\n",
      "********** Iteration 48 ************\n",
      "\tLast Episode Reward: -113\n",
      "********** Iteration 49 ************\n",
      "\tLast Episode Reward: -157\n",
      "********** Iteration 50 ************\n",
      "[[ 6.119652  -2.3150308 -8.624076 ]\n",
      " [-2.3150308  2.4697983  4.643305 ]\n",
      " [-8.624076   4.643305  17.215353 ]]\n",
      "\tLast Episode Reward: -115\n",
      "********** Iteration 51 ************\n",
      "\tLast Episode Reward: -130\n",
      "********** Iteration 52 ************\n",
      "\tLast Episode Reward: -120\n",
      "********** Iteration 53 ************\n",
      "\tLast Episode Reward: -81\n",
      "********** Iteration 54 ************\n",
      "\tLast Episode Reward: -168\n",
      "********** Iteration 55 ************\n",
      "[[ 4.397016  -1.9758393 -5.1167564]\n",
      " [-1.9758393  2.794152   3.99605  ]\n",
      " [-5.1167564  3.99605   10.982491 ]]\n",
      "\tLast Episode Reward: -85\n",
      "********** Iteration 56 ************\n",
      "\tLast Episode Reward: -99\n",
      "********** Iteration 57 ************\n",
      "\tLast Episode Reward: -97\n",
      "********** Iteration 58 ************\n",
      "\tLast Episode Reward: -95\n",
      "********** Iteration 59 ************\n",
      "\tLast Episode Reward: -110\n",
      "********** Iteration 60 ************\n",
      "[[ 3.4269438 -1.5359514 -3.0379825]\n",
      " [-1.5359514  2.8903122  3.1698813]\n",
      " [-3.0379825  3.1698813  7.243602 ]]\n",
      "\tLast Episode Reward: -91\n",
      "********** Iteration 61 ************\n",
      "\tLast Episode Reward: -90\n",
      "********** Iteration 62 ************\n",
      "\tLast Episode Reward: -83\n",
      "********** Iteration 63 ************\n",
      "\tLast Episode Reward: -89\n",
      "********** Iteration 64 ************\n",
      "\tLast Episode Reward: -90\n",
      "********** Iteration 65 ************\n",
      "[[ 2.8888025 -1.1313119 -1.7991555]\n",
      " [-1.1313119  2.8626864  2.407175 ]\n",
      " [-1.7991555  2.407175   4.9206533]]\n",
      "\tLast Episode Reward: -82\n",
      "********** Iteration 66 ************\n",
      "\tLast Episode Reward: -111\n",
      "********** Iteration 67 ************\n",
      "\tLast Episode Reward: -73\n",
      "********** Iteration 68 ************\n",
      "\tLast Episode Reward: -98\n",
      "********** Iteration 69 ************\n",
      "\tLast Episode Reward: -81\n",
      "********** Iteration 70 ************\n",
      "[[ 2.6087084  -0.82843685 -1.1193107 ]\n",
      " [-0.82843685  2.8110857   1.8519199 ]\n",
      " [-1.1193107   1.8519199   3.6238928 ]]\n",
      "\tLast Episode Reward: -123\n",
      "********** Iteration 71 ************\n",
      "\tLast Episode Reward: -81\n",
      "********** Iteration 72 ************\n",
      "\tLast Episode Reward: -105\n",
      "********** Iteration 73 ************\n",
      "\tLast Episode Reward: -104\n",
      "********** Iteration 74 ************\n",
      "\tLast Episode Reward: -90\n",
      "********** Iteration 75 ************\n",
      "[[ 2.461533  -0.5754746 -0.71225  ]\n",
      " [-0.5754746  2.6857452  1.3925585]\n",
      " [-0.71225    1.3925585  2.8035994]]\n",
      "\tLast Episode Reward: -139\n",
      "********** Iteration 76 ************\n",
      "\tLast Episode Reward: -80\n",
      "********** Iteration 77 ************\n",
      "\tLast Episode Reward: -124\n",
      "********** Iteration 78 ************\n",
      "\tLast Episode Reward: -150\n",
      "********** Iteration 79 ************\n",
      "\tLast Episode Reward: -121\n",
      "********** Iteration 80 ************\n",
      "[[ 2.338595   -0.37556118 -0.45256752]\n",
      " [-0.37556118  2.5784235   1.0411131 ]\n",
      " [-0.45256752  1.0411131   2.2891765 ]]\n",
      "\tLast Episode Reward: -87\n",
      "********** Iteration 81 ************\n",
      "\tLast Episode Reward: -93\n",
      "********** Iteration 82 ************\n",
      "\tLast Episode Reward: -124\n",
      "********** Iteration 83 ************\n",
      "\tLast Episode Reward: -76\n",
      "********** Iteration 84 ************\n",
      "\tLast Episode Reward: -104\n",
      "********** Iteration 85 ************\n",
      "[[ 2.2512977  -0.19656768 -0.2808236 ]\n",
      " [-0.19656768  2.4544168   0.76411647]\n",
      " [-0.2808236   0.76411647  1.9703052 ]]\n",
      "\tLast Episode Reward: -125\n",
      "********** Iteration 86 ************\n",
      "\tLast Episode Reward: -115\n",
      "********** Iteration 87 ************\n",
      "\tLast Episode Reward: -135\n",
      "********** Iteration 88 ************\n",
      "\tLast Episode Reward: -83\n",
      "********** Iteration 89 ************\n",
      "\tLast Episode Reward: -135\n",
      "********** Iteration 90 ************\n",
      "[[ 2.190418   -0.0658882  -0.16450948]\n",
      " [-0.0658882   2.382807    0.5621951 ]\n",
      " [-0.16450948  0.5621951   1.8096478 ]]\n",
      "\tLast Episode Reward: -103\n",
      "********** Iteration 91 ************\n",
      "\tLast Episode Reward: -125\n",
      "********** Iteration 92 ************\n",
      "\tLast Episode Reward: -107\n",
      "********** Iteration 93 ************\n",
      "\tLast Episode Reward: -168\n",
      "********** Iteration 94 ************\n",
      "\tLast Episode Reward: -93\n",
      "********** Iteration 95 ************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.1432269   0.06716754 -0.07827357]\n",
      " [ 0.06716754  2.2742634   0.3868028 ]\n",
      " [-0.07827357  0.3868028   1.6916862 ]]\n",
      "\tLast Episode Reward: -85\n",
      "********** Iteration 96 ************\n",
      "\tLast Episode Reward: -137\n",
      "********** Iteration 97 ************\n",
      "\tLast Episode Reward: -113\n",
      "********** Iteration 98 ************\n",
      "\tLast Episode Reward: -112\n",
      "********** Iteration 99 ************\n",
      "\tLast Episode Reward: -98\n",
      "********** Iteration 100 ************\n",
      "[[ 2.0996008   0.17108786 -0.02221721]\n",
      " [ 0.17108786  2.2097008   0.26909617]\n",
      " [-0.02221721  0.26909617  1.6343083 ]]\n",
      "\tLast Episode Reward: -120\n",
      "********** Iteration 101 ************\n",
      "\tLast Episode Reward: -134\n",
      "********** Iteration 102 ************\n",
      "\tLast Episode Reward: -113\n",
      "********** Iteration 103 ************\n",
      "\tLast Episode Reward: -89\n",
      "********** Iteration 104 ************\n",
      "\tLast Episode Reward: -109\n",
      "********** Iteration 105 ************\n",
      "[[2.0780826  0.26350945 0.03289714]\n",
      " [0.26350945 2.1333585  0.16832745]\n",
      " [0.03289714 0.16832745 1.6237141 ]]\n",
      "\tLast Episode Reward: -108\n",
      "********** Iteration 106 ************\n",
      "\tLast Episode Reward: -103\n",
      "********** Iteration 107 ************\n",
      "\tLast Episode Reward: -113\n",
      "********** Iteration 108 ************\n",
      "\tLast Episode Reward: -132\n",
      "********** Iteration 109 ************\n",
      "\tLast Episode Reward: -146\n",
      "********** Iteration 110 ************\n",
      "[[2.0672402  0.3375728  0.06075053]\n",
      " [0.3375728  2.0972676  0.1127299 ]\n",
      " [0.06075053 0.1127299  1.627286  ]]\n",
      "\tLast Episode Reward: -129\n",
      "********** Iteration 111 ************\n",
      "\tLast Episode Reward: -122\n",
      "********** Iteration 112 ************\n",
      "\tLast Episode Reward: -122\n",
      "********** Iteration 113 ************\n",
      "\tLast Episode Reward: -139\n",
      "********** Iteration 114 ************\n",
      "\tLast Episode Reward: -180\n",
      "********** Iteration 115 ************\n",
      "[[2.063469   0.40274334 0.08996971]\n",
      " [0.40274334 2.0209532  0.07224084]\n",
      " [0.08996971 0.07224084 1.6375163 ]]\n",
      "\tLast Episode Reward: -107\n",
      "********** Iteration 116 ************\n",
      "\tLast Episode Reward: -130\n",
      "********** Iteration 117 ************\n",
      "\tLast Episode Reward: -134\n",
      "********** Iteration 118 ************\n",
      "\tLast Episode Reward: -102\n",
      "********** Iteration 119 ************\n",
      "\tLast Episode Reward: -119\n",
      "********** Iteration 120 ************\n",
      "[[2.0521462  0.45004696 0.12650234]\n",
      " [0.45004696 2.0226412  0.06874567]\n",
      " [0.12650234 0.06874567 1.6526467 ]]\n",
      "\tLast Episode Reward: -109\n",
      "********** Iteration 121 ************\n",
      "\tLast Episode Reward: -174\n",
      "********** Iteration 122 ************\n",
      "\tLast Episode Reward: -97\n",
      "********** Iteration 123 ************\n",
      "\tLast Episode Reward: -102\n",
      "********** Iteration 124 ************\n",
      "\tLast Episode Reward: -157\n",
      "********** Iteration 125 ************\n",
      "[[2.0876603  0.48335737 0.156341  ]\n",
      " [0.48335737 1.9890959  0.05689223]\n",
      " [0.156341   0.05689223 1.7008736 ]]\n",
      "\tLast Episode Reward: -155\n",
      "********** Iteration 126 ************\n",
      "\tLast Episode Reward: -161\n",
      "********** Iteration 127 ************\n",
      "\tLast Episode Reward: -105\n",
      "********** Iteration 128 ************\n",
      "\tLast Episode Reward: -100\n",
      "********** Iteration 129 ************\n",
      "\tLast Episode Reward: -115\n",
      "********** Iteration 130 ************\n",
      "[[2.1044016  0.5144836  0.1618399 ]\n",
      " [0.5144836  1.9755323  0.07880723]\n",
      " [0.1618399  0.07880723 1.7208554 ]]\n",
      "\tLast Episode Reward: -85\n",
      "********** Iteration 131 ************\n",
      "\tLast Episode Reward: -115\n",
      "********** Iteration 132 ************\n",
      "\tLast Episode Reward: -151\n",
      "********** Iteration 133 ************\n",
      "\tLast Episode Reward: -162\n",
      "********** Iteration 134 ************\n",
      "\tLast Episode Reward: -263\n",
      "********** Iteration 135 ************\n",
      "[[2.0979946  0.54843056 0.19935688]\n",
      " [0.54843056 1.9664804  0.09106967]\n",
      " [0.19935688 0.09106967 1.749572  ]]\n",
      "\tLast Episode Reward: -116\n",
      "********** Iteration 136 ************\n",
      "\tLast Episode Reward: -193\n",
      "********** Iteration 137 ************\n",
      "\tLast Episode Reward: -144\n",
      "********** Iteration 138 ************\n",
      "\tLast Episode Reward: -112\n",
      "********** Iteration 139 ************\n",
      "\tLast Episode Reward: -127\n",
      "********** Iteration 140 ************\n",
      "[[2.128864   0.55453116 0.22440985]\n",
      " [0.55453116 1.9418842  0.10226985]\n",
      " [0.22440985 0.10226985 1.7762022 ]]\n",
      "\tLast Episode Reward: -155\n",
      "********** Iteration 141 ************\n",
      "\tLast Episode Reward: -97\n",
      "********** Iteration 142 ************\n",
      "\tLast Episode Reward: -168\n",
      "********** Iteration 143 ************\n",
      "\tLast Episode Reward: -207\n",
      "********** Iteration 144 ************\n",
      "\tLast Episode Reward: -141\n",
      "********** Iteration 145 ************\n",
      "[[2.1296415  0.55551064 0.21304035]\n",
      " [0.55551064 1.9521482  0.14033318]\n",
      " [0.21304035 0.14033318 1.8102915 ]]\n",
      "\tLast Episode Reward: -128\n",
      "********** Iteration 146 ************\n",
      "\tLast Episode Reward: -115\n",
      "********** Iteration 147 ************\n",
      "\tLast Episode Reward: -167\n",
      "********** Iteration 148 ************\n",
      "\tLast Episode Reward: -145\n",
      "********** Iteration 149 ************\n",
      "\tLast Episode Reward: -152\n",
      "********** Iteration 150 ************\n",
      "[[2.129552   0.5617027  0.23751411]\n",
      " [0.5617027  1.957766   0.16617316]\n",
      " [0.23751411 0.16617316 1.8237716 ]]\n",
      "\tLast Episode Reward: -164\n",
      "********** Iteration 151 ************\n",
      "\tLast Episode Reward: -160\n",
      "********** Iteration 152 ************\n",
      "\tLast Episode Reward: -147\n",
      "********** Iteration 153 ************\n",
      "\tLast Episode Reward: -162\n",
      "********** Iteration 154 ************\n",
      "\tLast Episode Reward: -139\n",
      "********** Iteration 155 ************\n",
      "[[2.1284654  0.5746847  0.27120137]\n",
      " [0.5746847  1.9369113  0.179494  ]\n",
      " [0.27120137 0.179494   1.8106015 ]]\n",
      "\tLast Episode Reward: -141\n",
      "********** Iteration 156 ************\n",
      "\tLast Episode Reward: -128\n",
      "********** Iteration 157 ************\n",
      "\tLast Episode Reward: -117\n",
      "********** Iteration 158 ************\n",
      "\tLast Episode Reward: -89\n",
      "********** Iteration 159 ************\n",
      "\tLast Episode Reward: -159\n",
      "********** Iteration 160 ************\n",
      "[[2.1574306  0.58628714 0.286342  ]\n",
      " [0.58628714 1.9323242  0.18427423]\n",
      " [0.286342   0.18427423 1.8245246 ]]\n",
      "\tLast Episode Reward: -151\n",
      "********** Iteration 161 ************\n",
      "\tLast Episode Reward: -122\n",
      "********** Iteration 162 ************\n",
      "\tLast Episode Reward: -147\n",
      "********** Iteration 163 ************\n",
      "\tLast Episode Reward: -151\n",
      "********** Iteration 164 ************\n",
      "\tLast Episode Reward: -142\n",
      "********** Iteration 165 ************\n",
      "[[2.1492953  0.60248923 0.26079974]\n",
      " [0.60248923 1.935748   0.2248011 ]\n",
      " [0.26079974 0.2248011  1.8706875 ]]\n",
      "\tLast Episode Reward: -129\n",
      "********** Iteration 166 ************\n",
      "\tLast Episode Reward: -119\n",
      "********** Iteration 167 ************\n",
      "\tLast Episode Reward: -133\n",
      "********** Iteration 168 ************\n",
      "\tLast Episode Reward: -115\n",
      "********** Iteration 169 ************\n",
      "\tLast Episode Reward: -115\n",
      "********** Iteration 170 ************\n",
      "[[2.1623588  0.60535705 0.28821635]\n",
      " [0.60535705 1.9352392  0.22342119]\n",
      " [0.28821635 0.22342119 1.8801001 ]]\n",
      "\tLast Episode Reward: -137\n",
      "********** Iteration 171 ************\n",
      "\tLast Episode Reward: -142\n",
      "********** Iteration 172 ************\n",
      "\tLast Episode Reward: -157\n",
      "********** Iteration 173 ************\n",
      "\tLast Episode Reward: -148\n",
      "********** Iteration 174 ************\n",
      "\tLast Episode Reward: -142\n",
      "********** Iteration 175 ************\n",
      "[[2.1403983  0.61197174 0.29244485]\n",
      " [0.61197174 1.9519624  0.24704662]\n",
      " [0.29244485 0.24704662 1.8725003 ]]\n",
      "\tLast Episode Reward: -86\n",
      "********** Iteration 176 ************\n",
      "\tLast Episode Reward: -137\n",
      "********** Iteration 177 ************\n",
      "\tLast Episode Reward: -107\n",
      "********** Iteration 178 ************\n",
      "\tLast Episode Reward: -144\n",
      "********** Iteration 179 ************\n",
      "\tLast Episode Reward: -154\n",
      "********** Iteration 180 ************\n",
      "[[2.1560702  0.61191094 0.2725935 ]\n",
      " [0.61191094 1.9513918  0.27083015]\n",
      " [0.2725935  0.27083015 1.9083837 ]]\n",
      "\tLast Episode Reward: -170\n",
      "********** Iteration 181 ************\n",
      "\tLast Episode Reward: -218\n",
      "********** Iteration 182 ************\n",
      "\tLast Episode Reward: -154\n",
      "********** Iteration 183 ************\n",
      "\tLast Episode Reward: -147\n",
      "********** Iteration 184 ************\n",
      "\tLast Episode Reward: -146\n",
      "********** Iteration 185 ************\n",
      "[[2.1496174  0.60844237 0.29301125]\n",
      " [0.60844237 1.9516566  0.28004938]\n",
      " [0.29301125 0.28004938 1.903349  ]]\n",
      "\tLast Episode Reward: -170\n",
      "********** Iteration 186 ************\n",
      "\tLast Episode Reward: -157\n",
      "********** Iteration 187 ************\n",
      "\tLast Episode Reward: -163\n",
      "********** Iteration 188 ************\n",
      "\tLast Episode Reward: -181\n",
      "********** Iteration 189 ************\n",
      "\tLast Episode Reward: -239\n",
      "********** Iteration 190 ************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1598458  0.62604177 0.32634324]\n",
      " [0.62604177 1.9174247  0.28688878]\n",
      " [0.32634324 0.28688878 1.9203987 ]]\n",
      "\tLast Episode Reward: -133\n",
      "********** Iteration 191 ************\n",
      "\tLast Episode Reward: -116\n",
      "********** Iteration 192 ************\n",
      "\tLast Episode Reward: -178\n",
      "********** Iteration 193 ************\n",
      "\tLast Episode Reward: -151\n",
      "********** Iteration 194 ************\n",
      "\tLast Episode Reward: -174\n",
      "********** Iteration 195 ************\n",
      "[[2.1641765  0.6216904  0.28282577]\n",
      " [0.6216904  1.9295766  0.33078527]\n",
      " [0.28282577 0.33078527 1.9205961 ]]\n",
      "\tLast Episode Reward: -157\n",
      "********** Iteration 196 ************\n",
      "\tLast Episode Reward: -158\n",
      "********** Iteration 197 ************\n",
      "\tLast Episode Reward: -107\n",
      "********** Iteration 198 ************\n",
      "\tLast Episode Reward: -148\n",
      "********** Iteration 199 ************\n",
      "\tLast Episode Reward: -148\n",
      "********** Iteration 200 ************\n",
      "[[2.1738806  0.62239695 0.30924943]\n",
      " [0.62239695 1.9292613  0.28798974]\n",
      " [0.30924943 0.28798974 1.9566933 ]]\n",
      "\tLast Episode Reward: -103\n",
      "********** Iteration 201 ************\n",
      "\tLast Episode Reward: -160\n",
      "********** Iteration 202 ************\n",
      "\tLast Episode Reward: -164\n",
      "********** Iteration 203 ************\n",
      "\tLast Episode Reward: -122\n",
      "********** Iteration 204 ************\n",
      "\tLast Episode Reward: -119\n",
      "********** Iteration 205 ************\n",
      "[[2.184543  0.6351909 0.2975021]\n",
      " [0.6351909 1.9384162 0.3149007]\n",
      " [0.2975021 0.3149007 1.9408134]]\n",
      "\tLast Episode Reward: -121\n",
      "********** Iteration 206 ************\n",
      "\tLast Episode Reward: -125\n",
      "********** Iteration 207 ************\n",
      "\tLast Episode Reward: -164\n",
      "********** Iteration 208 ************\n",
      "\tLast Episode Reward: -133\n",
      "********** Iteration 209 ************\n",
      "\tLast Episode Reward: -153\n",
      "********** Iteration 210 ************\n",
      "[[2.1639347  0.6424314  0.29899764]\n",
      " [0.6424314  1.9297047  0.31469268]\n",
      " [0.29899764 0.31469268 1.957166  ]]\n",
      "\tLast Episode Reward: -135\n",
      "********** Iteration 211 ************\n",
      "\tLast Episode Reward: -127\n",
      "********** Iteration 212 ************\n",
      "\tLast Episode Reward: -143\n",
      "********** Iteration 213 ************\n",
      "\tLast Episode Reward: -135\n",
      "********** Iteration 214 ************\n",
      "\tLast Episode Reward: -139\n",
      "********** Iteration 215 ************\n",
      "[[2.147542   0.65626585 0.2997008 ]\n",
      " [0.65626585 1.9738183  0.31697047]\n",
      " [0.2997008  0.31697047 1.9247892 ]]\n",
      "\tLast Episode Reward: -110\n",
      "********** Iteration 216 ************\n",
      "\tLast Episode Reward: -146\n",
      "********** Iteration 217 ************\n",
      "\tLast Episode Reward: -109\n",
      "********** Iteration 218 ************\n",
      "\tLast Episode Reward: -120\n",
      "********** Iteration 219 ************\n",
      "\tLast Episode Reward: -138\n",
      "********** Iteration 220 ************\n",
      "[[2.1471674  0.64834845 0.29825708]\n",
      " [0.64834845 1.9513553  0.36911902]\n",
      " [0.29825708 0.36911902 1.9742802 ]]\n",
      "\tLast Episode Reward: -168\n",
      "********** Iteration 221 ************\n",
      "\tLast Episode Reward: -116\n",
      "********** Iteration 222 ************\n",
      "\tLast Episode Reward: -110\n",
      "********** Iteration 223 ************\n",
      "\tLast Episode Reward: -124\n",
      "********** Iteration 224 ************\n",
      "\tLast Episode Reward: -176\n",
      "********** Iteration 225 ************\n",
      "[[2.150204   0.6446099  0.30805534]\n",
      " [0.6446099  1.9485173  0.3597324 ]\n",
      " [0.30805534 0.3597324  1.9711273 ]]\n",
      "\tLast Episode Reward: -130\n",
      "********** Iteration 226 ************\n",
      "\tLast Episode Reward: -189\n",
      "********** Iteration 227 ************\n",
      "\tLast Episode Reward: -226\n",
      "********** Iteration 228 ************\n",
      "\tLast Episode Reward: -133\n",
      "********** Iteration 229 ************\n",
      "\tLast Episode Reward: -117\n",
      "********** Iteration 230 ************\n",
      "[[2.1763468  0.63942003 0.30218613]\n",
      " [0.63942003 1.9244661  0.3446294 ]\n",
      " [0.30218613 0.3446294  1.9476781 ]]\n",
      "\tLast Episode Reward: -171\n",
      "********** Iteration 231 ************\n",
      "\tLast Episode Reward: -97\n",
      "********** Iteration 232 ************\n",
      "\tLast Episode Reward: -161\n",
      "********** Iteration 233 ************\n",
      "\tLast Episode Reward: -144\n",
      "********** Iteration 234 ************\n",
      "\tLast Episode Reward: -154\n",
      "********** Iteration 235 ************\n",
      "[[2.1662924  0.63366747 0.3437024 ]\n",
      " [0.63366747 1.9342397  0.32988602]\n",
      " [0.3437024  0.32988602 1.9650146 ]]\n",
      "\tLast Episode Reward: -172\n",
      "********** Iteration 236 ************\n",
      "\tLast Episode Reward: -208\n",
      "********** Iteration 237 ************\n",
      "\tLast Episode Reward: -230\n",
      "********** Iteration 238 ************\n",
      "\tLast Episode Reward: -128\n",
      "********** Iteration 239 ************\n",
      "\tLast Episode Reward: -146\n",
      "********** Iteration 240 ************\n",
      "[[2.162625  0.6367077 0.301346 ]\n",
      " [0.6367077 1.9752822 0.3655594]\n",
      " [0.301346  0.3655594 1.9684514]]\n",
      "\tLast Episode Reward: -127\n",
      "********** Iteration 241 ************\n",
      "\tLast Episode Reward: -120\n",
      "********** Iteration 242 ************\n",
      "\tLast Episode Reward: -130\n",
      "********** Iteration 243 ************\n",
      "\tLast Episode Reward: -185\n",
      "********** Iteration 244 ************\n",
      "\tLast Episode Reward: -102\n",
      "********** Iteration 245 ************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-298b034b9982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/stanford/research/current/koopman-lqr/code/train_utils.py\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent, env, max_timesteps, max_episodes, max_iters, max_seconds, n_transitions_between_updates, n_optim_steps_per_update, n_iters_per_p_update)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# optimize the model from collected data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_optim_steps_per_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miters_so_far\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_iters_per_p_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/stanford/research/current/koopman-lqr/code/klqr.py\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         })\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/stanford/research/current/koopman-lqr/klqr_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/stanford/research/current/koopman-lqr/klqr_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/stanford/research/current/koopman-lqr/klqr_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/stanford/research/current/koopman-lqr/klqr_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/stanford/research/current/koopman-lqr/klqr_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/stanford/research/current/koopman-lqr/klqr_env/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LQ-v0')\n",
    "agent = klqr(config,sess)\n",
    "agent.build_model()\n",
    "train_agent(agent,env,max_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(agent.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(agent.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0189447 ,  0.08293143,  0.15776801],\n",
       "       [ 0.08293143,  0.7517757 , -0.05224696],\n",
       "       [ 0.15776801, -0.05224696,  0.9286399 ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(agent.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.738761  , -0.03628063, -2.764276  ],\n",
       "       [-0.03628063, 14.90458   , -0.02685213],\n",
       "       [-2.764276  , -0.02685213,  8.3499365 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(agent.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
