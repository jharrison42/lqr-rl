{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# add gradient clipping?\n",
    "\n",
    "# TO TEST:\n",
    "# enforcing norm constraint on P\n",
    "# how to do exploration\n",
    "# whether this works at all?\n",
    "\n",
    "# carries out mat*v on all v in batch_v\n",
    "# mat = [n, m], batch_v = [batch_size, m], returns [batch_size, n]\n",
    "def batch_matmul(mat, batch_v):\n",
    "    return tf.transpose(tf.matmul(mat,tf.transpose(batch_v)))\n",
    "\n",
    "def summarize_matrix(name, matrix):\n",
    "    with tf.name_scope(name):\n",
    "        eigvals, _ = tf.linalg.eigh(matrix)\n",
    "        tf.summary.scalar('max_eig', tf.reduce_max(eigvals))\n",
    "        tf.summary.scalar('min_eig', tf.reduce_min(eigvals))\n",
    "        tf.summary.scalar('mean_eig', tf.reduce_mean(eigvals))\n",
    "\n",
    "class klqr:\n",
    "    # not currently doing value updates at varying rates\n",
    "    # not currently doing double Q learning (what would this look like?)\n",
    "    \n",
    "    def __init__(self,config,sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.x_dim = config['x_dim']\n",
    "        self.z_dim = config['z_dim']\n",
    "        self.a_dim = config['a_dim']\n",
    "        self.lr = config['lr']\n",
    "        self.horizon = config['horizon']\n",
    "        self.gamma = config['discount_rate']\n",
    "\n",
    "        \n",
    "        ou_theta = config['ou_theta']\n",
    "        ou_sigma = config['ou_sigma']\n",
    "        self.config = config\n",
    "        \n",
    "        # Ornstein-Uhlenbeck noise for exploration -- code from Yuke Zhu\n",
    "        self.noise_var = tf.Variable(tf.zeros([self.a_dim,1]))\n",
    "        noise_random = tf.random_normal([self.a_dim,1], stddev=ou_sigma)\n",
    "        self.noise = self.noise_var.assign_sub((ou_theta) * self.noise_var - noise_random)\n",
    "\n",
    "        self.max_riccati_updates = config['max_riccati_updates']\n",
    "        self.train_batch_size = config['train_batch_size']\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size=config['replay_buffer_size'])\n",
    "        \n",
    "        self.dynamics_weight = 1.0\n",
    "        self.cost_weight = 0.0\n",
    "        self.td_weight = 0.0\n",
    "        \n",
    "        self.experience_count = 0\n",
    "        \n",
    "        self.updates_so_far = 0\n",
    "        \n",
    "        self.run_count = 0\n",
    "        \n",
    "    def build_model(self):        \n",
    "\n",
    "        with tf.variable_scope('model',reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            self.x_ = tf.placeholder(tf.float32,shape=[None, self.x_dim])\n",
    "            self.xp_ = tf.placeholder(tf.float32,shape=[None, self.x_dim])\n",
    "            self.a_ = tf.placeholder(tf.float32,shape=[None, self.a_dim])\n",
    "            self.r_ = tf.placeholder(tf.float32,shape=[None])\n",
    "            \n",
    "            self.z = self.encoder(self.x_)\n",
    "            self.zp = self.encoder(self.xp_)\n",
    "\n",
    "            print('z shape:', self.z.get_shape())\n",
    "\n",
    "            #init R\n",
    "\n",
    "            self.R_asym = tf.get_variable('R_asym',shape=[self.a_dim,self.a_dim])\n",
    "    #         self.R_asym = tf.Variable(np.random.rand(self.a_dim,self.a_dim) - 0.5)\n",
    "\n",
    "            # working with Ra.T Ra so that inner product is norm(Rx) and not norm(R.T x)\n",
    "            self.R = tf.matmul(tf.linalg.transpose(self.R_asym),self.R_asym)\n",
    "\n",
    "            #init Q -- shape: z_dim * z_dim\n",
    "            self.Q_asym = tf.get_variable('Q_asym',shape=[self.z_dim,self.z_dim])\n",
    "            self.Q = tf.matmul(tf.linalg.transpose(self.Q_asym),self.Q_asym)\n",
    "\n",
    "            #init P -- shape: z_dim * z_dim\n",
    "            self.P = tf.get_variable('P',shape=[self.z_dim,self.z_dim],trainable=False, initializer=tf.initializers.identity)\n",
    "            self.P_asym = tf.linalg.transpose(tf.cholesky(self.P))\n",
    "\n",
    "            #init B -- shape: z_dim * u_dim\n",
    "            self.B = tf.get_variable('B',shape=[self.z_dim,self.a_dim])\n",
    "    #         self.B = tf.Variable(np.random.rand(self.z_dim,self.u_dim) - 0.5)\n",
    "\n",
    "            #init A -- shape: z_dim * z_dim\n",
    "            self.A = tf.get_variable('A',shape=[self.z_dim,self.z_dim])\n",
    "    #         self.A = tf.Variable(np.random.rand(self.z_dim,self.z_dim) - 0.5)\n",
    "\n",
    "            #define K -- shape: u_dim * z_dim\n",
    "            term1 = tf.matrix_inverse(self.R + tf.linalg.transpose(self.B) @ self.P @ self.B)\n",
    "            term2 = tf.linalg.transpose(self.B) @ self.P @ self.A\n",
    "            self.K = tf.stop_gradient( -term1 @ term2 )\n",
    "            self.policy_action = batch_matmul(self.K, self.z)\n",
    "            \n",
    "            # predict next state\n",
    "            self.zp_pred = batch_matmul(self.A, self.x_) + batch_matmul(self.B, self.a_)\n",
    "            \n",
    "                        \n",
    "            #make reward negative to convert to cost\n",
    "            self.bootstrapped_value = -self.r_ + self.gamma*tf.square(tf.norm(batch_matmul(self.P_asym, self.zp), axis=1))\n",
    "\n",
    "            action_cost = tf.square(tf.norm(batch_matmul(self.R_asym, self.a_), axis=1))#can simplify this by taking norm on other axis\n",
    "            state_cost = tf.square(tf.norm(batch_matmul(self.Q_asym, self.z), axis=1)) \n",
    "            #self.PABK = self.P_asym @ ( self.A + self.B @ self.K )\n",
    "            Vzp = tf.square(tf.norm(batch_matmul(self.P_asym, self.zp_pred), axis=1))\n",
    "            self.Qsa = action_cost + state_cost + Vzp\n",
    "            \n",
    "\n",
    "            # predict observed reward\n",
    "            self.r_pred = - action_cost - state_cost\n",
    "\n",
    "            self.td_loss = tf.reduce_mean(tf.square(self.bootstrapped_value - self.Qsa))\n",
    "            self.dynamics_loss = tf.reduce_mean(tf.square(self.xp_ - self.zp_pred))\n",
    "            self.cost_pred_loss = tf.reduce_mean(tf.square(self.r_pred - self.r_))\n",
    "            \n",
    "            \n",
    "            self.loss = self.td_weight*self.td_loss + self.dynamics_weight*self.dynamics_loss + self.cost_weight*self.cost_pred_loss\n",
    "            global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
    "            \n",
    "            # utilities for doing riccati recursion\n",
    "            self.reset_P_op = self.P.assign(tf.stop_gradient(self.Q))\n",
    "            self.riccati_update_op = self.P.assign(tf.stop_gradient(self.riccati_recursion_step()))\n",
    "            \n",
    "            # record summaries\n",
    "            tf.summary.scalar('dynamics_loss', self.dynamics_loss)\n",
    "            tf.summary.scalar('cost_pred_loss', self.cost_pred_loss)\n",
    "            tf.summary.scalar('td_loss', self.td_loss)\n",
    "            summarize_matrix('A', self.A)\n",
    "#             summarize_matrix('B', self.B)\n",
    "            summarize_matrix('Q', self.Q)\n",
    "            summarize_matrix('R', self.R)\n",
    "            summarize_matrix('P', self.P)\n",
    "            \n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.train_writer = tf.summary.FileWriter('summaries/'+str(time.time()))\n",
    "            \n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    def update_model(self):  \n",
    "        self.run_count += 1\n",
    "        #this function is mostly taken from Yuke's code\n",
    "        if self.replay_buffer.count() < self.train_batch_size:\n",
    "            return\n",
    "        \n",
    "        batch           = self.replay_buffer.getBatch(self.train_batch_size)\n",
    "        \n",
    "        states          = np.zeros((self.train_batch_size, self.x_dim))\n",
    "        rewards         = np.zeros((self.train_batch_size))\n",
    "        actions         = np.zeros((self.train_batch_size, self.a_dim))\n",
    "        next_states     = np.zeros((self.train_batch_size, self.x_dim))\n",
    "\n",
    "        for k, (s0, a, r, s1, done) in enumerate(batch):\n",
    "            #currently throwing away done states; should fix this\n",
    "            states[k] = s0\n",
    "            rewards[k] = r\n",
    "            actions[k] = a\n",
    "            next_states[k] = s1\n",
    "            # check terminal state\n",
    "#             if not done:\n",
    "#                 next_states[k] = s1\n",
    "#                 next_state_mask[k] = 1\n",
    "\n",
    "        summary, _ = self.sess.run([self.merged, self.train_op],\n",
    "        {\n",
    "        self.x_:  states,\n",
    "        self.xp_: next_states,\n",
    "        self.a_:  actions,\n",
    "        self.r_:  rewards\n",
    "        })\n",
    "    \n",
    "        self.train_writer.add_summary(summary, self.updates_so_far)\n",
    "        self.updates_so_far += 1\n",
    "    \n",
    "        #possibly update target via Riccati recursion? or do standard target separation? \n",
    "    \n",
    "    def riccati_recursion_step(self):\n",
    "#         ABK = self.A + self.B @ self.K\n",
    "#         APA = tf.transpose(ABK) @ self.P @ ABK \n",
    "#         return self.Q + tf.transpose(self.K) @ self.R @ self.K + self.gamma*APA\n",
    "        newP =  self.Q + tf.transpose(self.A) @ self.P @ self.A - tf.transpose(self.A) @ self.P @ self.B @ tf.matrix_inverse(self.R + tf.transpose(self.B) @ self.P @ self.B ) @ tf.transpose(self.B) @ tf.transpose(self.P) @ self.A\n",
    "        return 0.5*(newP + tf.transpose(newP))\n",
    "        \n",
    "    def update_P(self):\n",
    "#         print('updating P')\n",
    "#         reset_q_op = \n",
    "#         self.P = tf.identity(self.Q)\n",
    "#         for k in range(self.max_riccati_updates):\n",
    "#             #do Riccati backup in tensorflow oh god why\n",
    "#             ABK = self.A + tf.matmul(self.B,self.K)\n",
    "#             APA = tf.matmul(tf.matmul(tf.transpose(ABK),self.P),ABK) #\n",
    "#             self.P = self.Q + tf.matmul(tf.matmul(tf.transpose(self.K),self.R),self.K) + self.gamma*APA\n",
    "        \n",
    "#         self.P_asym = tf.transpose(tf.cholesky(self.P))\n",
    "        sess.run(self.reset_P_op)\n",
    "        for k in range(self.max_riccati_updates):\n",
    "            sess.run(self.riccati_update_op)\n",
    "        \n",
    "        print('Q:',sess.run(self.Q))\n",
    "        print('R:',sess.run(self.R))\n",
    "        print('A:',sess.run(self.A))\n",
    "        print('B:',sess.run(self.B))\n",
    "        print('P:',sess.run(self.P))\n",
    "            #TODO add a termination criterion for norm of Riccati update difference?\n",
    "    \n",
    "    def pi(self,x,explore=True):\n",
    "        self.experience_count += 1\n",
    "        x = np.reshape(x,(1,self.x_dim))\n",
    "        a,w = self.sess.run([self.policy_action,self.noise], {self.x_: x})\n",
    "\n",
    "        a = w.T\n",
    "#         else:\n",
    "#             a = a + w.T if explore else a\n",
    "\n",
    "        \n",
    "        return a.tolist()[0]\n",
    "        \n",
    "    def store_experience(self,s,a,r,sp,done):\n",
    "        # currently storing experience for every iteration\n",
    "        self.replay_buffer.add(s, a, r, sp, done)\n",
    "    \n",
    "    def encoder(self,x,name=\"encoder\",batch_norm=False):\n",
    "#         layer_sizes = self.config['encoder_layers']\n",
    "#         with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "#             inp = x\n",
    "#             for units in layer_sizes: \n",
    "#                 inp = tf.layers.dense(inputs=inp, units=units,activation=tf.nn.relu)\n",
    "\n",
    "#             z = tf.layers.dense(inputs=inp, units=self.z_dim,activation=None)\n",
    "\n",
    "#         if batch_norm:\n",
    "#             z = tf.layers.batch_normalization(z)\n",
    "\n",
    "        return x #z\n",
    "\n",
    "class ReplayBuffer:\n",
    "    # taken from Yuke Zhu's Q learning implementation\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        # random draw N\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, next_action, done):\n",
    "        new_experience = (state, action, reward, next_action, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "          self.buffer.append(new_experience)\n",
    "          self.num_experiences += 1\n",
    "        else:\n",
    "          self.buffer.popleft()\n",
    "          self.buffer.append(new_experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulates the agent acting in env, yielding every N steps\n",
    "# (decouples episode reseting mechanics from the training alg)\n",
    "def experience_generator(agent, env, N):\n",
    "    s = env.reset()\n",
    "    n_steps = 0\n",
    "    n_eps = 0\n",
    "    last_cum_rew = 0\n",
    "    cum_rew = 0\n",
    "    while True:\n",
    "        n_steps += 1\n",
    "        a = agent.pi(s)\n",
    "#         print('itr:',n_steps)\n",
    "#         print('a:',a)\n",
    "#         print('s:',s,'\\n')\n",
    "        sp, r, done,_ = env.step(a)\n",
    "        cum_rew += r\n",
    "        if done:\n",
    "            n_eps += 1\n",
    "            last_cum_rew = cum_rew\n",
    "            cum_rew = 0\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            agent.store_experience(s, a, r, sp, done)\n",
    "            s = sp\n",
    "\n",
    "        if n_steps % N == 0:\n",
    "            print('itr:',n_steps)\n",
    "            print('a:',a)\n",
    "            print('s:',s,'\\n')\n",
    "            yield (n_steps, n_eps, last_cum_rew)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def train_agent(agent, env,\n",
    "                max_timesteps=0, max_episodes=0, max_iters=0, max_seconds=0, # time constraint\n",
    "                n_transitions_between_updates=100,\n",
    "                n_optim_steps_per_update=10,\n",
    "                n_iters_per_p_update=1,\n",
    "                ):\n",
    "\n",
    "    # run an episode, and feed data to model\n",
    "    episodes_so_far = 0\n",
    "    timesteps_so_far = 0\n",
    "    iters_so_far = 0\n",
    "    tstart = time.time()\n",
    "\n",
    "    assert sum([max_iters>0, max_timesteps>0, max_episodes>0, max_seconds>0])==1, \"Only one time constraint permitted\"\n",
    "\n",
    "    exp_gen = experience_generator(agent, env, n_transitions_between_updates)\n",
    "\n",
    "    while True:\n",
    "        iters_so_far += 1\n",
    "        if max_timesteps and timesteps_so_far >= max_timesteps:\n",
    "            break\n",
    "        elif max_episodes and episodes_so_far >= max_episodes:\n",
    "            break\n",
    "        elif max_iters and iters_so_far >= max_iters:\n",
    "            break\n",
    "        elif max_seconds and time.time() - tstart >= max_seconds:\n",
    "            break\n",
    "\n",
    "        print(\"********** Iteration %i ************\"%iters_so_far)\n",
    "\n",
    "        # gather experience\n",
    "        timesteps_so_far, episodes_so_far, last_cum_rew = exp_gen.__next__()\n",
    "\n",
    "        # optimize the model from collected data:\n",
    "        for i in range(n_optim_steps_per_update):\n",
    "            agent.update_model()\n",
    "\n",
    "        if iters_so_far % n_iters_per_p_update == 0:\n",
    "#             pass\n",
    "            agent.update_P()\n",
    "\n",
    "        print(\"\\tLast Episode Reward: %d\"%last_cum_rew)\n",
    "        # add other logging stuff here\n",
    "        # add saving checkpoints here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yml','r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: (?, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'klqr' object has no attribute 'cost_pred_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-298b034b9982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LQ-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9b1328365e02>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# record summaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dynamics_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cost_pred_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_pred_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'td_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0msummarize_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'klqr' object has no attribute 'cost_pred_loss'"
     ]
    }
   ],
   "source": [
    "env = gym.make('LQ-v0')\n",
    "agent = klqr(config,sess)\n",
    "agent.build_model()\n",
    "train_agent(agent,env,max_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
