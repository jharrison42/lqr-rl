{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# add gradient clipping?\n",
    "\n",
    "# TO TEST:\n",
    "# enforcing norm constraint on P\n",
    "# how to do exploration\n",
    "# whether this works at all?\n",
    "\n",
    "# carries out mat*v on all v in batch_v\n",
    "# mat = [n, m], batch_v = [batch_size, m], returns [batch_size, n]\n",
    "def batch_matmul(mat, batch_v):\n",
    "    return tf.transpose(tf.matmul(mat,tf.transpose(batch_v)))\n",
    "\n",
    "def summarize_matrix(name, matrix):\n",
    "    with tf.name_scope(name):\n",
    "        eigvals, _ = tf.linalg.eigh(matrix)\n",
    "        tf.summary.scalar('max_eig', tf.reduce_max(eigvals))\n",
    "        tf.summary.scalar('min_eig', tf.reduce_min(eigvals))\n",
    "        tf.summary.scalar('mean_eig', tf.reduce_mean(eigvals))\n",
    "\n",
    "class klqr:\n",
    "    # not currently doing value updates at varying rates\n",
    "    # not currently doing double Q learning (what would this look like?)\n",
    "    \n",
    "    def __init__(self,config,sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.x_dim = config['x_dim']\n",
    "        self.z_dim = config['z_dim']\n",
    "        self.a_dim = config['a_dim']\n",
    "        self.lr = config['lr']\n",
    "        self.horizon = config['horizon']\n",
    "        self.gamma = config['discount_rate']\n",
    "\n",
    "        \n",
    "        ou_theta = config['ou_theta']\n",
    "        ou_sigma = config['ou_sigma']\n",
    "        self.config = config\n",
    "        \n",
    "        # Ornstein-Uhlenbeck noise for exploration -- code from Yuke Zhu\n",
    "        self.noise_var = tf.Variable(tf.zeros([self.a_dim,1]))\n",
    "        noise_random = tf.random_normal([self.a_dim,1], stddev=ou_sigma)\n",
    "        self.noise = self.noise_var.assign_sub((ou_theta) * self.noise_var - noise_random)\n",
    "\n",
    "        self.max_riccati_updates = config['max_riccati_updates']\n",
    "        self.train_batch_size = config['train_batch_size']\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size=config['replay_buffer_size'])\n",
    "        \n",
    "        self.dynamics_weight = 1.0\n",
    "        self.cost_weight = 1.0\n",
    "        self.td_weight = 0.0\n",
    "        \n",
    "        self.experience_count = 0\n",
    "        \n",
    "        self.updates_so_far = 0\n",
    "        \n",
    "    def build_model(self):        \n",
    "\n",
    "        with tf.variable_scope('model',reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            self.x_ = tf.placeholder(tf.float32,shape=[None, self.x_dim])\n",
    "            self.xp_ = tf.placeholder(tf.float32,shape=[None, self.x_dim])\n",
    "            self.a_ = tf.placeholder(tf.float32,shape=[None, self.a_dim])\n",
    "            self.r_ = tf.placeholder(tf.float32,shape=[None])\n",
    "            \n",
    "            self.z = self.encoder(self.x_)\n",
    "            self.zp = self.encoder(self.xp_)\n",
    "\n",
    "            print('z shape:', self.z.get_shape())\n",
    "\n",
    "            #init R\n",
    "\n",
    "            self.R_asym = tf.get_variable('R_asym',shape=[self.a_dim,self.a_dim])\n",
    "    #         self.R_asym = tf.Variable(np.random.rand(self.a_dim,self.a_dim) - 0.5)\n",
    "\n",
    "            # working with Ra.T Ra so that inner product is norm(Rx) and not norm(R.T x)\n",
    "            self.R = tf.matmul(tf.linalg.transpose(self.R_asym),self.R_asym)\n",
    "\n",
    "            #init Q -- shape: z_dim * z_dim\n",
    "            self.Q_asym = tf.get_variable('Q_asym',shape=[self.z_dim,self.z_dim])\n",
    "            self.Q = tf.matmul(tf.linalg.transpose(self.Q_asym),self.Q_asym)\n",
    "\n",
    "            #init P -- shape: z_dim * z_dim\n",
    "            self.P = tf.get_variable('P',shape=[self.z_dim,self.z_dim],trainable=False,initializer=tf.initializers.identity)\n",
    "            self.P_asym = tf.linalg.transpose(tf.cholesky(self.P))\n",
    "\n",
    "            #init B -- shape: z_dim * u_dim\n",
    "            self.B = tf.get_variable('B',shape=[self.z_dim,self.a_dim])\n",
    "    #         self.B = tf.Variable(np.random.rand(self.z_dim,self.u_dim) - 0.5)\n",
    "\n",
    "            #init A -- shape: z_dim * z_dim\n",
    "            self.A = tf.get_variable('A',shape=[self.z_dim,self.z_dim])\n",
    "    #         self.A = tf.Variable(np.random.rand(self.z_dim,self.z_dim) - 0.5)\n",
    "\n",
    "            #define K -- shape: u_dim * z_dim\n",
    "            #term1 = tf.matrix_inverse(self.R + tf.matmul(tf.matmul(tf.transpose(self.B),self.Q),self.B))\n",
    "            term1 = tf.matrix_inverse(self.R + tf.linalg.transpose(self.B) @ self.P @ self.B)\n",
    "            term2 = tf.linalg.transpose(self.B) @ self.P @ self.A\n",
    "            self.K = term1 @ term2\n",
    "            self.policy_action = batch_matmul(self.K, self.z) # tf.transpose(tf.matmul(self.K,tf.transpose(self.z)))\n",
    "                        \n",
    "            #make reward negative to convert to cost\n",
    "            self.bootstrapped_value = -self.r_ + self.gamma*tf.square(tf.norm(batch_matmul(self.P_asym, self.zp), axis=1))\n",
    "\n",
    "            action_cost = tf.square(tf.norm(batch_matmul(self.R_asym, self.a_), axis=1))#can simplify this by taking norm on other axis\n",
    "            state_cost = tf.square(tf.norm(batch_matmul(self.Q_asym, self.z), axis=1)) \n",
    "            self.PABK = self.P_asym @ ( self.A + self.B @ self.K )\n",
    "            Vzp = tf.square(tf.norm(batch_matmul(self.PABK, self.zp), axis=1))\n",
    "            self.Qsa = action_cost + state_cost + Vzp\n",
    "            \n",
    "            # predict next state\n",
    "            self.zp_pred = batch_matmul(self.A, self.z) + batch_matmul(self.B, self.a_)\n",
    "            \n",
    "            # predict observed reward\n",
    "            self.r_pred = - action_cost - state_cost\n",
    "            print(self.r_pred.get_shape())\n",
    "\n",
    "            self.td_loss = tf.reduce_mean(tf.square(self.bootstrapped_value - self.Qsa))\n",
    "            self.dynamics_loss = tf.reduce_mean(tf.square(self.zp - self.zp_pred))\n",
    "            self.cost_pred_loss = tf.reduce_mean(tf.square(self.r_pred - self.r_))\n",
    "            \n",
    "            \n",
    "            self.loss = self.td_weight*self.td_loss + self.dynamics_weight*self.dynamics_loss + self.cost_weight*self.cost_pred_loss\n",
    "            global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
    "            \n",
    "            # utilities for doing riccati recursion\n",
    "            self.reset_P_op = self.P.assign(self.Q)\n",
    "            self.riccati_update_op = self.P.assign(self.riccati_recursion_step())\n",
    "            \n",
    "            # record summaries\n",
    "            tf.summary.scalar('dynamics_loss', self.dynamics_loss)\n",
    "            tf.summary.scalar('cost_pred_loss', self.cost_pred_loss)\n",
    "            tf.summary.scalar('td_loss', self.td_loss)\n",
    "            summarize_matrix('A', self.A)\n",
    "#             summarize_matrix('B', self.B)\n",
    "            summarize_matrix('Q', self.Q)\n",
    "            summarize_matrix('R', self.R)\n",
    "            summarize_matrix('P', self.P)\n",
    "            \n",
    "            self.merged = tf.summary.merge_all()\n",
    "            self.train_writer = tf.summary.FileWriter('summaries')\n",
    "            \n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    def update_model(self):        \n",
    "        #this function is mostly taken from Yuke's code\n",
    "        if self.replay_buffer.count() < self.train_batch_size:\n",
    "            return\n",
    "        \n",
    "        batch           = self.replay_buffer.getBatch(self.train_batch_size)\n",
    "        \n",
    "        states          = np.zeros((self.train_batch_size, self.x_dim))\n",
    "        rewards         = np.zeros((self.train_batch_size))\n",
    "        actions         = np.zeros((self.train_batch_size, self.a_dim))\n",
    "        next_states     = np.zeros((self.train_batch_size, self.x_dim))\n",
    "\n",
    "        for k, (s0, a, r, s1, done) in enumerate(batch):\n",
    "            #currently throwing away done states; should fix this\n",
    "            states[k] = s0\n",
    "            rewards[k] = r\n",
    "            actions[k] = a\n",
    "            next_states[k] = s1\n",
    "            # check terminal state\n",
    "#             if not done:\n",
    "#                 next_states[k] = s1\n",
    "#                 next_state_mask[k] = 1\n",
    "\n",
    "        summary, _ = self.sess.run([self.merged, self.train_op],\n",
    "        {\n",
    "        self.x_:  states,\n",
    "        self.xp_: next_states,\n",
    "        self.a_:  actions,\n",
    "        self.r_:  rewards\n",
    "        })\n",
    "    \n",
    "        self.train_writer.add_summary(summary, self.updates_so_far)\n",
    "        self.updates_so_far += 1\n",
    "    \n",
    "        #possibly update target via Riccati recursion? or do standard target separation? \n",
    "    \n",
    "    def riccati_recursion_step(self):\n",
    "#         ABK = self.A + self.B @ self.K\n",
    "#         APA = tf.transpose(ABK) @ self.P @ ABK \n",
    "#         return self.Q + tf.transpose(self.K) @ self.R @ self.K + self.gamma*APA\n",
    "        return self.Q + tf.transpose(self.A) @ self.P @ self.A - tf.transpose(self.A) @ self.P @ self.B @ tf.matrix_inverse(self.R + tf.transpose(self.B) @ self.P @ self.B ) @ tf.transpose(self.P @ self.B) @ self.A\n",
    "    \n",
    "    def update_P(self):\n",
    "#         print('updating P')\n",
    "#         reset_q_op = \n",
    "#         self.P = tf.identity(self.Q)\n",
    "#         for k in range(self.max_riccati_updates):\n",
    "#             #do Riccati backup in tensorflow oh god why\n",
    "#             ABK = self.A + tf.matmul(self.B,self.K)\n",
    "#             APA = tf.matmul(tf.matmul(tf.transpose(ABK),self.P),ABK) #\n",
    "#             self.P = self.Q + tf.matmul(tf.matmul(tf.transpose(self.K),self.R),self.K) + self.gamma*APA\n",
    "        \n",
    "#         self.P_asym = tf.transpose(tf.cholesky(self.P))\n",
    "        sess.run(self.reset_P_op)\n",
    "        for k in range(self.max_riccati_updates):\n",
    "            sess.run(self.riccati_update_op)\n",
    "        \n",
    "\n",
    "        print(sess.run(self.P))\n",
    "            #TODO add a termination criterion for norm of Riccati update difference?\n",
    "    \n",
    "    def pi(self,x,explore=True):\n",
    "        self.experience_count += 1\n",
    "        x = np.reshape(x,(1,3))\n",
    "        \n",
    "        a,w = self.sess.run([self.policy_action,self.noise], {self.x_: x})\n",
    "        \n",
    "        a = a + w if explore else a\n",
    "        # TODO check the dimension of the output of this\n",
    "        return [a[0,0]]\n",
    "        \n",
    "    def store_experience(self,s,a,r,sp,done):\n",
    "        # currently storing experience for every iteration\n",
    "        self.replay_buffer.add(s, a, r, sp, done)\n",
    "    \n",
    "    def encoder(self,x,name=\"encoder\",batch_norm=False):\n",
    "        layer_sizes = self.config['encoder_layers']\n",
    "        with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "            inp = x\n",
    "            for units in layer_sizes: \n",
    "                inp = tf.layers.dense(inputs=inp, units=units,activation=tf.nn.relu)\n",
    "\n",
    "            z = tf.layers.dense(inputs=inp, units=self.z_dim,activation=None)\n",
    "\n",
    "        if batch_norm:\n",
    "            z = tf.layers.batch_normalization(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "class ReplayBuffer:\n",
    "    # taken from Yuke Zhu's Q learning implementation\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        # random draw N\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, next_action, done):\n",
    "        new_experience = (state, action, reward, next_action, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "          self.buffer.append(new_experience)\n",
    "          self.num_experiences += 1\n",
    "        else:\n",
    "          self.buffer.popleft()\n",
    "          self.buffer.append(new_experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulates the agent acting in env, yielding every N steps\n",
    "# (decouples episode reseting mechanics from the training alg)\n",
    "def experience_generator(agent, env, N):\n",
    "    s = env.reset()\n",
    "    n_steps = 0\n",
    "    n_eps = 0\n",
    "    last_cum_rew = 0\n",
    "    cum_rew = 0\n",
    "    while True:\n",
    "        n_steps += 1\n",
    "        a = agent.pi(s)\n",
    "        sp, r, done,_ = env.step(a)\n",
    "        cum_rew += r\n",
    "        if done:\n",
    "            n_eps += 1\n",
    "            last_cum_rew = cum_rew\n",
    "            cum_rew = 0\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            agent.store_experience(s, a, r, sp, done)\n",
    "            s = sp\n",
    "\n",
    "        if n_steps % N == 0:\n",
    "            yield (n_steps, n_eps, last_cum_rew)\n",
    "\n",
    "\n",
    "\n",
    "def train_agent(agent, env,\n",
    "                max_timesteps=0, max_episodes=0, max_iters=0, max_seconds=0, # time constraint\n",
    "                n_transitions_between_updates=100,\n",
    "                n_optim_steps_per_update=100,\n",
    "                n_iters_per_p_update=100,\n",
    "                ):\n",
    "\n",
    "    # run an episode, and feed data to model\n",
    "    episodes_so_far = 0\n",
    "    timesteps_so_far = 0\n",
    "    iters_so_far = 0\n",
    "    tstart = time.time()\n",
    "\n",
    "    assert sum([max_iters>0, max_timesteps>0, max_episodes>0, max_seconds>0])==1, \"Only one time constraint permitted\"\n",
    "\n",
    "    exp_gen = experience_generator(agent, env, n_transitions_between_updates)\n",
    "\n",
    "    while True:\n",
    "        iters_so_far += 1\n",
    "        if max_timesteps and timesteps_so_far >= max_timesteps:\n",
    "            break\n",
    "        elif max_episodes and episodes_so_far >= max_episodes:\n",
    "            break\n",
    "        elif max_iters and iters_so_far >= max_iters:\n",
    "            break\n",
    "        elif max_seconds and time.time() - tstart >= max_seconds:\n",
    "            break\n",
    "\n",
    "        print(\"********** Iteration %i ************\"%iters_so_far)\n",
    "\n",
    "        # gather experience\n",
    "        timesteps_so_far, episodes_so_far, last_cum_rew = exp_gen.__next__()\n",
    "\n",
    "        # optimize the model from collected data:\n",
    "        for i in range(n_optim_steps_per_update):\n",
    "            agent.update_model()\n",
    "\n",
    "        if iters_so_far % n_iters_per_p_update == 0:\n",
    "            agent.update_P()\n",
    "\n",
    "        print(\"\\tLast Episode Reward: %d\"%last_cum_rew)\n",
    "        # add other logging stuff here\n",
    "        # add saving checkpoints here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yml','r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "z shape: (?, 6)\n",
      "(?,)\n",
      "********** Iteration 1 ************\n",
      "\tLast Episode Reward: 0\n",
      "********** Iteration 2 ************\n",
      "\tLast Episode Reward: -1256\n",
      "********** Iteration 3 ************\n",
      "\tLast Episode Reward: -1256\n",
      "********** Iteration 4 ************\n",
      "\tLast Episode Reward: -956\n",
      "********** Iteration 5 ************\n",
      "\tLast Episode Reward: -956\n",
      "********** Iteration 6 ************\n",
      "\tLast Episode Reward: -1813\n",
      "********** Iteration 7 ************\n",
      "\tLast Episode Reward: -1813\n",
      "********** Iteration 8 ************\n",
      "\tLast Episode Reward: -944\n",
      "********** Iteration 9 ************\n",
      "\tLast Episode Reward: -944\n",
      "********** Iteration 10 ************\n",
      "\tLast Episode Reward: -974\n",
      "********** Iteration 11 ************\n",
      "\tLast Episode Reward: -974\n",
      "********** Iteration 12 ************\n",
      "\tLast Episode Reward: -1155\n",
      "********** Iteration 13 ************\n",
      "\tLast Episode Reward: -1155\n",
      "********** Iteration 14 ************\n",
      "\tLast Episode Reward: -872\n",
      "********** Iteration 15 ************\n",
      "\tLast Episode Reward: -872\n",
      "********** Iteration 16 ************\n",
      "\tLast Episode Reward: -988\n",
      "********** Iteration 17 ************\n",
      "\tLast Episode Reward: -988\n",
      "********** Iteration 18 ************\n",
      "\tLast Episode Reward: -1260\n",
      "********** Iteration 19 ************\n",
      "\tLast Episode Reward: -1260\n",
      "********** Iteration 20 ************\n",
      "\tLast Episode Reward: -1369\n",
      "********** Iteration 21 ************\n",
      "\tLast Episode Reward: -1369\n",
      "********** Iteration 22 ************\n",
      "\tLast Episode Reward: -1098\n",
      "********** Iteration 23 ************\n",
      "\tLast Episode Reward: -1098\n",
      "********** Iteration 24 ************\n",
      "\tLast Episode Reward: -1328\n",
      "********** Iteration 25 ************\n",
      "\tLast Episode Reward: -1328\n",
      "********** Iteration 26 ************\n",
      "\tLast Episode Reward: -1176\n",
      "********** Iteration 27 ************\n",
      "\tLast Episode Reward: -1176\n",
      "********** Iteration 28 ************\n",
      "\tLast Episode Reward: -910\n",
      "********** Iteration 29 ************\n",
      "\tLast Episode Reward: -910\n",
      "********** Iteration 30 ************\n",
      "\tLast Episode Reward: -1082\n",
      "********** Iteration 31 ************\n",
      "\tLast Episode Reward: -1082\n",
      "********** Iteration 32 ************\n",
      "\tLast Episode Reward: -870\n",
      "********** Iteration 33 ************\n",
      "\tLast Episode Reward: -870\n",
      "********** Iteration 34 ************\n",
      "\tLast Episode Reward: -775\n",
      "********** Iteration 35 ************\n",
      "\tLast Episode Reward: -775\n",
      "********** Iteration 36 ************\n",
      "\tLast Episode Reward: -903\n",
      "********** Iteration 37 ************\n",
      "\tLast Episode Reward: -903\n",
      "********** Iteration 38 ************\n",
      "\tLast Episode Reward: -859\n",
      "********** Iteration 39 ************\n",
      "\tLast Episode Reward: -859\n",
      "********** Iteration 40 ************\n",
      "\tLast Episode Reward: -1325\n",
      "********** Iteration 41 ************\n",
      "\tLast Episode Reward: -1325\n",
      "********** Iteration 42 ************\n",
      "\tLast Episode Reward: -1428\n",
      "********** Iteration 43 ************\n",
      "\tLast Episode Reward: -1428\n",
      "********** Iteration 44 ************\n",
      "\tLast Episode Reward: -1310\n",
      "********** Iteration 45 ************\n",
      "\tLast Episode Reward: -1310\n",
      "********** Iteration 46 ************\n",
      "\tLast Episode Reward: -929\n",
      "********** Iteration 47 ************\n",
      "\tLast Episode Reward: -929\n",
      "********** Iteration 48 ************\n",
      "\tLast Episode Reward: -1542\n",
      "********** Iteration 49 ************\n",
      "\tLast Episode Reward: -1542\n",
      "********** Iteration 50 ************\n",
      "\tLast Episode Reward: -1446\n",
      "********** Iteration 51 ************\n",
      "\tLast Episode Reward: -1446\n",
      "********** Iteration 52 ************\n",
      "\tLast Episode Reward: -756\n",
      "********** Iteration 53 ************\n",
      "\tLast Episode Reward: -756\n",
      "********** Iteration 54 ************\n",
      "\tLast Episode Reward: -1578\n",
      "********** Iteration 55 ************\n",
      "\tLast Episode Reward: -1578\n",
      "********** Iteration 56 ************\n",
      "\tLast Episode Reward: -1169\n",
      "********** Iteration 57 ************\n",
      "\tLast Episode Reward: -1169\n",
      "********** Iteration 58 ************\n",
      "\tLast Episode Reward: -1524\n",
      "********** Iteration 59 ************\n",
      "\tLast Episode Reward: -1524\n",
      "********** Iteration 60 ************\n",
      "\tLast Episode Reward: -1575\n",
      "********** Iteration 61 ************\n",
      "\tLast Episode Reward: -1575\n",
      "********** Iteration 62 ************\n",
      "\tLast Episode Reward: -867\n",
      "********** Iteration 63 ************\n",
      "\tLast Episode Reward: -867\n",
      "********** Iteration 64 ************\n",
      "\tLast Episode Reward: -1142\n",
      "********** Iteration 65 ************\n",
      "\tLast Episode Reward: -1142\n",
      "********** Iteration 66 ************\n",
      "\tLast Episode Reward: -1179\n",
      "********** Iteration 67 ************\n",
      "\tLast Episode Reward: -1179\n",
      "********** Iteration 68 ************\n",
      "\tLast Episode Reward: -1301\n",
      "********** Iteration 69 ************\n",
      "\tLast Episode Reward: -1301\n",
      "********** Iteration 70 ************\n",
      "\tLast Episode Reward: -1284\n",
      "********** Iteration 71 ************\n",
      "\tLast Episode Reward: -1284\n",
      "********** Iteration 72 ************\n",
      "\tLast Episode Reward: -1115\n",
      "********** Iteration 73 ************\n",
      "\tLast Episode Reward: -1115\n",
      "********** Iteration 74 ************\n",
      "\tLast Episode Reward: -1015\n",
      "********** Iteration 75 ************\n",
      "\tLast Episode Reward: -1015\n",
      "********** Iteration 76 ************\n",
      "\tLast Episode Reward: -1031\n",
      "********** Iteration 77 ************\n",
      "\tLast Episode Reward: -1031\n",
      "********** Iteration 78 ************\n",
      "\tLast Episode Reward: -1245\n",
      "********** Iteration 79 ************\n",
      "\tLast Episode Reward: -1245\n",
      "********** Iteration 80 ************\n",
      "\tLast Episode Reward: -1585\n",
      "********** Iteration 81 ************\n",
      "\tLast Episode Reward: -1585\n",
      "********** Iteration 82 ************\n",
      "\tLast Episode Reward: -1096\n",
      "********** Iteration 83 ************\n",
      "\tLast Episode Reward: -1096\n",
      "********** Iteration 84 ************\n",
      "\tLast Episode Reward: -1474\n",
      "********** Iteration 85 ************\n",
      "\tLast Episode Reward: -1474\n",
      "********** Iteration 86 ************\n",
      "\tLast Episode Reward: -1180\n",
      "********** Iteration 87 ************\n",
      "\tLast Episode Reward: -1180\n",
      "********** Iteration 88 ************\n",
      "\tLast Episode Reward: -1110\n",
      "********** Iteration 89 ************\n",
      "\tLast Episode Reward: -1110\n",
      "********** Iteration 90 ************\n",
      "\tLast Episode Reward: -1315\n",
      "********** Iteration 91 ************\n",
      "\tLast Episode Reward: -1315\n",
      "********** Iteration 92 ************\n",
      "\tLast Episode Reward: -1510\n",
      "********** Iteration 93 ************\n",
      "\tLast Episode Reward: -1510\n",
      "********** Iteration 94 ************\n",
      "\tLast Episode Reward: -1159\n",
      "********** Iteration 95 ************\n",
      "\tLast Episode Reward: -1159\n",
      "********** Iteration 96 ************\n",
      "\tLast Episode Reward: -989\n",
      "********** Iteration 97 ************\n",
      "\tLast Episode Reward: -989\n",
      "********** Iteration 98 ************\n",
      "\tLast Episode Reward: -1073\n",
      "********** Iteration 99 ************\n",
      "\tLast Episode Reward: -1073\n",
      "********** Iteration 100 ************\n",
      "[[ 8.3744904e+02 -4.5790161e+02  3.0338647e+02 -4.3168356e+02\n",
      "  -3.3443554e+01  4.1266489e+00]\n",
      " [-4.5790161e+02  2.5816010e+02 -1.6617624e+02  2.3541075e+02\n",
      "   2.5534142e+01 -2.4639397e+00]\n",
      " [ 3.0338657e+02 -1.6617624e+02  1.1525147e+02 -1.5770282e+02\n",
      "  -9.2492943e+00  2.4164665e+00]\n",
      " [-4.3168365e+02  2.3541077e+02 -1.5770280e+02  2.2522954e+02\n",
      "   1.5525487e+01 -2.5161793e+00]\n",
      " [-3.3443432e+01  2.5534126e+01 -9.2493095e+00  1.5525481e+01\n",
      "   1.2639817e+01 -1.3757381e-01]\n",
      " [ 4.1266365e+00 -2.4639509e+00  2.4164748e+00 -2.5161889e+00\n",
      "  -1.3756630e-01  1.9364492e+00]]\n",
      "\tLast Episode Reward: -977\n",
      "********** Iteration 101 ************\n",
      "\tLast Episode Reward: -977\n",
      "********** Iteration 102 ************\n",
      "\tLast Episode Reward: -1465\n",
      "********** Iteration 103 ************\n",
      "\tLast Episode Reward: -1465\n",
      "********** Iteration 104 ************\n",
      "\tLast Episode Reward: -1586\n",
      "********** Iteration 105 ************\n",
      "\tLast Episode Reward: -1586\n",
      "********** Iteration 106 ************\n",
      "\tLast Episode Reward: -1584\n",
      "********** Iteration 107 ************\n",
      "\tLast Episode Reward: -1584\n",
      "********** Iteration 108 ************\n",
      "\tLast Episode Reward: -1548\n",
      "********** Iteration 109 ************\n",
      "\tLast Episode Reward: -1548\n",
      "********** Iteration 110 ************\n",
      "\tLast Episode Reward: -1540\n",
      "********** Iteration 111 ************\n",
      "\tLast Episode Reward: -1540\n",
      "********** Iteration 112 ************\n",
      "\tLast Episode Reward: -1506\n",
      "********** Iteration 113 ************\n",
      "\tLast Episode Reward: -1506\n",
      "********** Iteration 114 ************\n",
      "\tLast Episode Reward: -1490\n",
      "********** Iteration 115 ************\n",
      "\tLast Episode Reward: -1490\n",
      "********** Iteration 116 ************\n",
      "\tLast Episode Reward: -1391\n",
      "********** Iteration 117 ************\n",
      "\tLast Episode Reward: -1391\n",
      "********** Iteration 118 ************\n",
      "\tLast Episode Reward: -1645\n",
      "********** Iteration 119 ************\n",
      "\tLast Episode Reward: -1645\n",
      "********** Iteration 120 ************\n",
      "\tLast Episode Reward: -1635\n",
      "********** Iteration 121 ************\n",
      "\tLast Episode Reward: -1635\n",
      "********** Iteration 122 ************\n",
      "\tLast Episode Reward: -1562\n",
      "********** Iteration 123 ************\n",
      "\tLast Episode Reward: -1562\n",
      "********** Iteration 124 ************\n",
      "\tLast Episode Reward: -1358\n",
      "********** Iteration 125 ************\n",
      "\tLast Episode Reward: -1358\n",
      "********** Iteration 126 ************\n",
      "\tLast Episode Reward: -1560\n",
      "********** Iteration 127 ************\n",
      "\tLast Episode Reward: -1560\n",
      "********** Iteration 128 ************\n",
      "\tLast Episode Reward: -1027\n",
      "********** Iteration 129 ************\n",
      "\tLast Episode Reward: -1027\n",
      "********** Iteration 130 ************\n",
      "\tLast Episode Reward: -1560\n",
      "********** Iteration 131 ************\n",
      "\tLast Episode Reward: -1560\n",
      "********** Iteration 132 ************\n",
      "\tLast Episode Reward: -1603\n",
      "********** Iteration 133 ************\n",
      "\tLast Episode Reward: -1603\n",
      "********** Iteration 134 ************\n",
      "\tLast Episode Reward: -1546\n",
      "********** Iteration 135 ************\n",
      "\tLast Episode Reward: -1546\n",
      "********** Iteration 136 ************\n",
      "\tLast Episode Reward: -1466\n",
      "********** Iteration 137 ************\n",
      "\tLast Episode Reward: -1466\n",
      "********** Iteration 138 ************\n",
      "\tLast Episode Reward: -1423\n",
      "********** Iteration 139 ************\n",
      "\tLast Episode Reward: -1423\n",
      "********** Iteration 140 ************\n",
      "\tLast Episode Reward: -1527\n",
      "********** Iteration 141 ************\n",
      "\tLast Episode Reward: -1527\n",
      "********** Iteration 142 ************\n",
      "\tLast Episode Reward: -946\n",
      "********** Iteration 143 ************\n",
      "\tLast Episode Reward: -946\n",
      "********** Iteration 144 ************\n",
      "\tLast Episode Reward: -1198\n",
      "********** Iteration 145 ************\n",
      "\tLast Episode Reward: -1198\n",
      "********** Iteration 146 ************\n",
      "\tLast Episode Reward: -1539\n",
      "********** Iteration 147 ************\n",
      "\tLast Episode Reward: -1539\n",
      "********** Iteration 148 ************\n",
      "\tLast Episode Reward: -1658\n",
      "********** Iteration 149 ************\n",
      "\tLast Episode Reward: -1658\n",
      "********** Iteration 150 ************\n",
      "\tLast Episode Reward: -1289\n",
      "********** Iteration 151 ************\n",
      "\tLast Episode Reward: -1289\n",
      "********** Iteration 152 ************\n",
      "\tLast Episode Reward: -1619\n",
      "********** Iteration 153 ************\n",
      "\tLast Episode Reward: -1619\n",
      "********** Iteration 154 ************\n",
      "\tLast Episode Reward: -1400\n",
      "********** Iteration 155 ************\n",
      "\tLast Episode Reward: -1400\n",
      "********** Iteration 156 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 157 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 158 ************\n",
      "\tLast Episode Reward: -1647\n",
      "********** Iteration 159 ************\n",
      "\tLast Episode Reward: -1647\n",
      "********** Iteration 160 ************\n",
      "\tLast Episode Reward: -1427\n",
      "********** Iteration 161 ************\n",
      "\tLast Episode Reward: -1427\n",
      "********** Iteration 162 ************\n",
      "\tLast Episode Reward: -1472\n",
      "********** Iteration 163 ************\n",
      "\tLast Episode Reward: -1472\n",
      "********** Iteration 164 ************\n",
      "\tLast Episode Reward: -1333\n",
      "********** Iteration 165 ************\n",
      "\tLast Episode Reward: -1333\n",
      "********** Iteration 166 ************\n",
      "\tLast Episode Reward: -1273\n",
      "********** Iteration 167 ************\n",
      "\tLast Episode Reward: -1273\n",
      "********** Iteration 168 ************\n",
      "\tLast Episode Reward: -1475\n",
      "********** Iteration 169 ************\n",
      "\tLast Episode Reward: -1475\n",
      "********** Iteration 170 ************\n",
      "\tLast Episode Reward: -1528\n",
      "********** Iteration 171 ************\n",
      "\tLast Episode Reward: -1528\n",
      "********** Iteration 172 ************\n",
      "\tLast Episode Reward: -1482\n",
      "********** Iteration 173 ************\n",
      "\tLast Episode Reward: -1482\n",
      "********** Iteration 174 ************\n",
      "\tLast Episode Reward: -1365\n",
      "********** Iteration 175 ************\n",
      "\tLast Episode Reward: -1365\n",
      "********** Iteration 176 ************\n",
      "\tLast Episode Reward: -1466\n",
      "********** Iteration 177 ************\n",
      "\tLast Episode Reward: -1466\n",
      "********** Iteration 178 ************\n",
      "\tLast Episode Reward: -1502\n",
      "********** Iteration 179 ************\n",
      "\tLast Episode Reward: -1502\n",
      "********** Iteration 180 ************\n",
      "\tLast Episode Reward: -1644\n",
      "********** Iteration 181 ************\n",
      "\tLast Episode Reward: -1644\n",
      "********** Iteration 182 ************\n",
      "\tLast Episode Reward: -1308\n",
      "********** Iteration 183 ************\n",
      "\tLast Episode Reward: -1308\n",
      "********** Iteration 184 ************\n",
      "\tLast Episode Reward: -1566\n",
      "********** Iteration 185 ************\n",
      "\tLast Episode Reward: -1566\n",
      "********** Iteration 186 ************\n",
      "\tLast Episode Reward: -1246\n",
      "********** Iteration 187 ************\n",
      "\tLast Episode Reward: -1246\n",
      "********** Iteration 188 ************\n",
      "\tLast Episode Reward: -1369\n",
      "********** Iteration 189 ************\n",
      "\tLast Episode Reward: -1369\n",
      "********** Iteration 190 ************\n",
      "\tLast Episode Reward: -1464\n",
      "********** Iteration 191 ************\n",
      "\tLast Episode Reward: -1464\n",
      "********** Iteration 192 ************\n",
      "\tLast Episode Reward: -1493\n",
      "********** Iteration 193 ************\n",
      "\tLast Episode Reward: -1493\n",
      "********** Iteration 194 ************\n",
      "\tLast Episode Reward: -1587\n",
      "********** Iteration 195 ************\n",
      "\tLast Episode Reward: -1587\n",
      "********** Iteration 196 ************\n",
      "\tLast Episode Reward: -1501\n",
      "********** Iteration 197 ************\n",
      "\tLast Episode Reward: -1501\n",
      "********** Iteration 198 ************\n",
      "\tLast Episode Reward: -1467\n",
      "********** Iteration 199 ************\n",
      "\tLast Episode Reward: -1467\n",
      "********** Iteration 200 ************\n",
      "[[ 96.9539    -49.387108   34.239628  -50.513527   -5.6906433   5.92653  ]\n",
      " [-49.387142   34.956696  -14.34444    26.168137   13.332829   -5.5542407]\n",
      " [ 34.23958   -14.3444     18.973139  -18.339142    4.5916224   1.0513252]\n",
      " [-50.513527   26.168118  -18.339167   28.866276    1.7075253  -3.1518118]\n",
      " [ -5.6907005  13.332859    4.5915976   1.7075559  16.359398   -4.255161 ]\n",
      " [  5.92655    -5.5542483   1.051337   -3.1518216  -4.2551575   3.6033268]]\n",
      "\tLast Episode Reward: -1351\n",
      "********** Iteration 201 ************\n",
      "\tLast Episode Reward: -1351\n",
      "********** Iteration 202 ************\n",
      "\tLast Episode Reward: -1494\n",
      "********** Iteration 203 ************\n",
      "\tLast Episode Reward: -1494\n",
      "********** Iteration 204 ************\n",
      "\tLast Episode Reward: -1429\n",
      "********** Iteration 205 ************\n",
      "\tLast Episode Reward: -1429\n",
      "********** Iteration 206 ************\n",
      "\tLast Episode Reward: -1507\n",
      "********** Iteration 207 ************\n",
      "\tLast Episode Reward: -1507\n",
      "********** Iteration 208 ************\n",
      "\tLast Episode Reward: -1071\n",
      "********** Iteration 209 ************\n",
      "\tLast Episode Reward: -1071\n",
      "********** Iteration 210 ************\n",
      "\tLast Episode Reward: -1376\n",
      "********** Iteration 211 ************\n",
      "\tLast Episode Reward: -1376\n",
      "********** Iteration 212 ************\n",
      "\tLast Episode Reward: -1198\n",
      "********** Iteration 213 ************\n",
      "\tLast Episode Reward: -1198\n",
      "********** Iteration 214 ************\n",
      "\tLast Episode Reward: -1487\n",
      "********** Iteration 215 ************\n",
      "\tLast Episode Reward: -1487\n",
      "********** Iteration 216 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 217 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 218 ************\n",
      "\tLast Episode Reward: -1499\n",
      "********** Iteration 219 ************\n",
      "\tLast Episode Reward: -1499\n",
      "********** Iteration 220 ************\n",
      "\tLast Episode Reward: -1137\n",
      "********** Iteration 221 ************\n",
      "\tLast Episode Reward: -1137\n",
      "********** Iteration 222 ************\n",
      "\tLast Episode Reward: -1555\n",
      "********** Iteration 223 ************\n",
      "\tLast Episode Reward: -1555\n",
      "********** Iteration 224 ************\n",
      "\tLast Episode Reward: -1491\n",
      "********** Iteration 225 ************\n",
      "\tLast Episode Reward: -1491\n",
      "********** Iteration 226 ************\n",
      "\tLast Episode Reward: -1215\n",
      "********** Iteration 227 ************\n",
      "\tLast Episode Reward: -1215\n",
      "********** Iteration 228 ************\n",
      "\tLast Episode Reward: -1498\n",
      "********** Iteration 229 ************\n",
      "\tLast Episode Reward: -1498\n",
      "********** Iteration 230 ************\n",
      "\tLast Episode Reward: -1526\n",
      "********** Iteration 231 ************\n",
      "\tLast Episode Reward: -1526\n",
      "********** Iteration 232 ************\n",
      "\tLast Episode Reward: -1479\n",
      "********** Iteration 233 ************\n",
      "\tLast Episode Reward: -1479\n",
      "********** Iteration 234 ************\n",
      "\tLast Episode Reward: -1393\n",
      "********** Iteration 235 ************\n",
      "\tLast Episode Reward: -1393\n",
      "********** Iteration 236 ************\n",
      "\tLast Episode Reward: -1532\n",
      "********** Iteration 237 ************\n",
      "\tLast Episode Reward: -1532\n",
      "********** Iteration 238 ************\n",
      "\tLast Episode Reward: -1385\n",
      "********** Iteration 239 ************\n",
      "\tLast Episode Reward: -1385\n",
      "********** Iteration 240 ************\n",
      "\tLast Episode Reward: -960\n",
      "********** Iteration 241 ************\n",
      "\tLast Episode Reward: -960\n",
      "********** Iteration 242 ************\n",
      "\tLast Episode Reward: -1500\n",
      "********** Iteration 243 ************\n",
      "\tLast Episode Reward: -1500\n",
      "********** Iteration 244 ************\n",
      "\tLast Episode Reward: -1337\n",
      "********** Iteration 245 ************\n",
      "\tLast Episode Reward: -1337\n",
      "********** Iteration 246 ************\n",
      "\tLast Episode Reward: -1449\n",
      "********** Iteration 247 ************\n",
      "\tLast Episode Reward: -1449\n",
      "********** Iteration 248 ************\n",
      "\tLast Episode Reward: -1524\n",
      "********** Iteration 249 ************\n",
      "\tLast Episode Reward: -1524\n",
      "********** Iteration 250 ************\n",
      "\tLast Episode Reward: -1284\n",
      "********** Iteration 251 ************\n",
      "\tLast Episode Reward: -1284\n",
      "********** Iteration 252 ************\n",
      "\tLast Episode Reward: -1509\n",
      "********** Iteration 253 ************\n",
      "\tLast Episode Reward: -1509\n",
      "********** Iteration 254 ************\n",
      "\tLast Episode Reward: -1656\n",
      "********** Iteration 255 ************\n",
      "\tLast Episode Reward: -1656\n",
      "********** Iteration 256 ************\n",
      "\tLast Episode Reward: -1500\n",
      "********** Iteration 257 ************\n",
      "\tLast Episode Reward: -1500\n",
      "********** Iteration 258 ************\n",
      "\tLast Episode Reward: -1433\n",
      "********** Iteration 259 ************\n",
      "\tLast Episode Reward: -1433\n",
      "********** Iteration 260 ************\n",
      "\tLast Episode Reward: -830\n",
      "********** Iteration 261 ************\n",
      "\tLast Episode Reward: -830\n",
      "********** Iteration 262 ************\n",
      "\tLast Episode Reward: -1467\n",
      "********** Iteration 263 ************\n",
      "\tLast Episode Reward: -1467\n",
      "********** Iteration 264 ************\n",
      "\tLast Episode Reward: -1512\n",
      "********** Iteration 265 ************\n",
      "\tLast Episode Reward: -1512\n",
      "********** Iteration 266 ************\n",
      "\tLast Episode Reward: -1279\n",
      "********** Iteration 267 ************\n",
      "\tLast Episode Reward: -1279\n",
      "********** Iteration 268 ************\n",
      "\tLast Episode Reward: -1486\n",
      "********** Iteration 269 ************\n",
      "\tLast Episode Reward: -1486\n",
      "********** Iteration 270 ************\n",
      "\tLast Episode Reward: -1500\n",
      "********** Iteration 271 ************\n",
      "\tLast Episode Reward: -1500\n",
      "********** Iteration 272 ************\n",
      "\tLast Episode Reward: -1443\n",
      "********** Iteration 273 ************\n",
      "\tLast Episode Reward: -1443\n",
      "********** Iteration 274 ************\n",
      "\tLast Episode Reward: -1450\n",
      "********** Iteration 275 ************\n",
      "\tLast Episode Reward: -1450\n",
      "********** Iteration 276 ************\n",
      "\tLast Episode Reward: -1273\n",
      "********** Iteration 277 ************\n",
      "\tLast Episode Reward: -1273\n",
      "********** Iteration 278 ************\n",
      "\tLast Episode Reward: -1181\n",
      "********** Iteration 279 ************\n",
      "\tLast Episode Reward: -1181\n",
      "********** Iteration 280 ************\n",
      "\tLast Episode Reward: -1457\n",
      "********** Iteration 281 ************\n",
      "\tLast Episode Reward: -1457\n",
      "********** Iteration 282 ************\n",
      "\tLast Episode Reward: -1486\n",
      "********** Iteration 283 ************\n",
      "\tLast Episode Reward: -1486\n",
      "********** Iteration 284 ************\n",
      "\tLast Episode Reward: -1365\n",
      "********** Iteration 285 ************\n",
      "\tLast Episode Reward: -1365\n",
      "********** Iteration 286 ************\n",
      "\tLast Episode Reward: -1509\n",
      "********** Iteration 287 ************\n",
      "\tLast Episode Reward: -1509\n",
      "********** Iteration 288 ************\n",
      "\tLast Episode Reward: -1307\n",
      "********** Iteration 289 ************\n",
      "\tLast Episode Reward: -1307\n",
      "********** Iteration 290 ************\n",
      "\tLast Episode Reward: -1014\n",
      "********** Iteration 291 ************\n",
      "\tLast Episode Reward: -1014\n",
      "********** Iteration 292 ************\n",
      "\tLast Episode Reward: -1437\n",
      "********** Iteration 293 ************\n",
      "\tLast Episode Reward: -1437\n",
      "********** Iteration 294 ************\n",
      "\tLast Episode Reward: -1449\n",
      "********** Iteration 295 ************\n",
      "\tLast Episode Reward: -1449\n",
      "********** Iteration 296 ************\n",
      "\tLast Episode Reward: -1340\n",
      "********** Iteration 297 ************\n",
      "\tLast Episode Reward: -1340\n",
      "********** Iteration 298 ************\n",
      "\tLast Episode Reward: -1493\n",
      "********** Iteration 299 ************\n",
      "\tLast Episode Reward: -1493\n",
      "********** Iteration 300 ************\n",
      "[[ 652.3347   -388.3582    190.78807  -345.9254   -102.919395   63.719322]\n",
      " [-388.35825   236.96553  -114.271324  206.3069     65.07691   -38.619377]\n",
      " [ 190.78813  -114.27133    58.340855 -101.44801   -30.09391    19.370054]\n",
      " [-345.92566   206.30702  -101.448074  185.96556    53.25925   -34.03993 ]\n",
      " [-102.91912    65.07676   -30.093801   53.259064   20.883968  -10.746891]\n",
      " [  63.719093  -38.61925    19.369976  -34.039803  -10.74689     8.306079]]\n",
      "\tLast Episode Reward: -1499\n",
      "********** Iteration 301 ************\n",
      "\tLast Episode Reward: -1499\n",
      "********** Iteration 302 ************\n",
      "\tLast Episode Reward: -1278\n",
      "********** Iteration 303 ************\n",
      "\tLast Episode Reward: -1278\n",
      "********** Iteration 304 ************\n",
      "\tLast Episode Reward: -1494\n",
      "********** Iteration 305 ************\n",
      "\tLast Episode Reward: -1494\n",
      "********** Iteration 306 ************\n",
      "\tLast Episode Reward: -1650\n",
      "********** Iteration 307 ************\n",
      "\tLast Episode Reward: -1650\n",
      "********** Iteration 308 ************\n",
      "\tLast Episode Reward: -1522\n",
      "********** Iteration 309 ************\n",
      "\tLast Episode Reward: -1522\n",
      "********** Iteration 310 ************\n",
      "\tLast Episode Reward: -1504\n",
      "********** Iteration 311 ************\n",
      "\tLast Episode Reward: -1504\n",
      "********** Iteration 312 ************\n",
      "\tLast Episode Reward: -1652\n",
      "********** Iteration 313 ************\n",
      "\tLast Episode Reward: -1652\n",
      "********** Iteration 314 ************\n",
      "\tLast Episode Reward: -1494\n",
      "********** Iteration 315 ************\n",
      "\tLast Episode Reward: -1494\n",
      "********** Iteration 316 ************\n",
      "\tLast Episode Reward: -1443\n",
      "********** Iteration 317 ************\n",
      "\tLast Episode Reward: -1443\n",
      "********** Iteration 318 ************\n",
      "\tLast Episode Reward: -1386\n",
      "********** Iteration 319 ************\n",
      "\tLast Episode Reward: -1386\n",
      "********** Iteration 320 ************\n",
      "\tLast Episode Reward: -1574\n",
      "********** Iteration 321 ************\n",
      "\tLast Episode Reward: -1574\n",
      "********** Iteration 322 ************\n",
      "\tLast Episode Reward: -1528\n",
      "********** Iteration 323 ************\n",
      "\tLast Episode Reward: -1528\n",
      "********** Iteration 324 ************\n",
      "\tLast Episode Reward: -1284\n",
      "********** Iteration 325 ************\n",
      "\tLast Episode Reward: -1284\n",
      "********** Iteration 326 ************\n",
      "\tLast Episode Reward: -1212\n",
      "********** Iteration 327 ************\n",
      "\tLast Episode Reward: -1212\n",
      "********** Iteration 328 ************\n",
      "\tLast Episode Reward: -1605\n",
      "********** Iteration 329 ************\n",
      "\tLast Episode Reward: -1605\n",
      "********** Iteration 330 ************\n",
      "\tLast Episode Reward: -1402\n",
      "********** Iteration 331 ************\n",
      "\tLast Episode Reward: -1402\n",
      "********** Iteration 332 ************\n",
      "\tLast Episode Reward: -1447\n",
      "********** Iteration 333 ************\n",
      "\tLast Episode Reward: -1447\n",
      "********** Iteration 334 ************\n",
      "\tLast Episode Reward: -593\n",
      "********** Iteration 335 ************\n",
      "\tLast Episode Reward: -593\n",
      "********** Iteration 336 ************\n",
      "\tLast Episode Reward: -1525\n",
      "********** Iteration 337 ************\n",
      "\tLast Episode Reward: -1525\n",
      "********** Iteration 338 ************\n",
      "\tLast Episode Reward: -1254\n",
      "********** Iteration 339 ************\n",
      "\tLast Episode Reward: -1254\n",
      "********** Iteration 340 ************\n",
      "\tLast Episode Reward: -1242\n",
      "********** Iteration 341 ************\n",
      "\tLast Episode Reward: -1242\n",
      "********** Iteration 342 ************\n",
      "\tLast Episode Reward: -1523\n",
      "********** Iteration 343 ************\n",
      "\tLast Episode Reward: -1523\n",
      "********** Iteration 344 ************\n",
      "\tLast Episode Reward: -1006\n",
      "********** Iteration 345 ************\n",
      "\tLast Episode Reward: -1006\n",
      "********** Iteration 346 ************\n",
      "\tLast Episode Reward: -1471\n",
      "********** Iteration 347 ************\n",
      "\tLast Episode Reward: -1471\n",
      "********** Iteration 348 ************\n",
      "\tLast Episode Reward: -1337\n",
      "********** Iteration 349 ************\n",
      "\tLast Episode Reward: -1337\n",
      "********** Iteration 350 ************\n",
      "\tLast Episode Reward: -1200\n",
      "********** Iteration 351 ************\n",
      "\tLast Episode Reward: -1200\n",
      "********** Iteration 352 ************\n",
      "\tLast Episode Reward: -1370\n",
      "********** Iteration 353 ************\n",
      "\tLast Episode Reward: -1370\n",
      "********** Iteration 354 ************\n",
      "\tLast Episode Reward: -1136\n",
      "********** Iteration 355 ************\n",
      "\tLast Episode Reward: -1136\n",
      "********** Iteration 356 ************\n",
      "\tLast Episode Reward: -1482\n",
      "********** Iteration 357 ************\n",
      "\tLast Episode Reward: -1482\n",
      "********** Iteration 358 ************\n",
      "\tLast Episode Reward: -1607\n",
      "********** Iteration 359 ************\n",
      "\tLast Episode Reward: -1607\n",
      "********** Iteration 360 ************\n",
      "\tLast Episode Reward: -1201\n",
      "********** Iteration 361 ************\n",
      "\tLast Episode Reward: -1201\n",
      "********** Iteration 362 ************\n",
      "\tLast Episode Reward: -1194\n",
      "********** Iteration 363 ************\n",
      "\tLast Episode Reward: -1194\n",
      "********** Iteration 364 ************\n",
      "\tLast Episode Reward: -1276\n",
      "********** Iteration 365 ************\n",
      "\tLast Episode Reward: -1276\n",
      "********** Iteration 366 ************\n",
      "\tLast Episode Reward: -1578\n",
      "********** Iteration 367 ************\n",
      "\tLast Episode Reward: -1578\n",
      "********** Iteration 368 ************\n",
      "\tLast Episode Reward: -1208\n",
      "********** Iteration 369 ************\n",
      "\tLast Episode Reward: -1208\n",
      "********** Iteration 370 ************\n",
      "\tLast Episode Reward: -972\n",
      "********** Iteration 371 ************\n",
      "\tLast Episode Reward: -972\n",
      "********** Iteration 372 ************\n",
      "\tLast Episode Reward: -1353\n",
      "********** Iteration 373 ************\n",
      "\tLast Episode Reward: -1353\n",
      "********** Iteration 374 ************\n",
      "\tLast Episode Reward: -1501\n",
      "********** Iteration 375 ************\n",
      "\tLast Episode Reward: -1501\n",
      "********** Iteration 376 ************\n",
      "\tLast Episode Reward: -1516\n",
      "********** Iteration 377 ************\n",
      "\tLast Episode Reward: -1516\n",
      "********** Iteration 378 ************\n",
      "\tLast Episode Reward: -1536\n",
      "********** Iteration 379 ************\n",
      "\tLast Episode Reward: -1536\n",
      "********** Iteration 380 ************\n",
      "\tLast Episode Reward: -1115\n",
      "********** Iteration 381 ************\n",
      "\tLast Episode Reward: -1115\n",
      "********** Iteration 382 ************\n",
      "\tLast Episode Reward: -1252\n",
      "********** Iteration 383 ************\n",
      "\tLast Episode Reward: -1252\n",
      "********** Iteration 384 ************\n",
      "\tLast Episode Reward: -1226\n",
      "********** Iteration 385 ************\n",
      "\tLast Episode Reward: -1226\n",
      "********** Iteration 386 ************\n",
      "\tLast Episode Reward: -1522\n",
      "********** Iteration 387 ************\n",
      "\tLast Episode Reward: -1522\n",
      "********** Iteration 388 ************\n",
      "\tLast Episode Reward: -1298\n",
      "********** Iteration 389 ************\n",
      "\tLast Episode Reward: -1298\n",
      "********** Iteration 390 ************\n",
      "\tLast Episode Reward: -1162\n",
      "********** Iteration 391 ************\n",
      "\tLast Episode Reward: -1162\n",
      "********** Iteration 392 ************\n",
      "\tLast Episode Reward: -1155\n",
      "********** Iteration 393 ************\n",
      "\tLast Episode Reward: -1155\n",
      "********** Iteration 394 ************\n",
      "\tLast Episode Reward: -1513\n",
      "********** Iteration 395 ************\n",
      "\tLast Episode Reward: -1513\n",
      "********** Iteration 396 ************\n",
      "\tLast Episode Reward: -1188\n",
      "********** Iteration 397 ************\n",
      "\tLast Episode Reward: -1188\n",
      "********** Iteration 398 ************\n",
      "\tLast Episode Reward: -1252\n",
      "********** Iteration 399 ************\n",
      "\tLast Episode Reward: -1252\n",
      "********** Iteration 400 ************\n",
      "[[140.97122    -51.743805    50.6442     -83.47656     20.97572\n",
      "   -3.3289366 ]\n",
      " [-51.945675    23.800472   -19.198853    30.808882    -4.650887\n",
      "    0.7176616 ]\n",
      " [ 50.614204   -19.110147    21.287746   -32.163258    10.7296915\n",
      "   -2.7530077 ]\n",
      " [-83.82085     30.810266   -32.314342    60.529823   -26.312557\n",
      "   10.415881  ]\n",
      " [ 21.251093    -4.712965    10.841469   -26.416546    25.239727\n",
      "  -13.611612  ]\n",
      " [ -3.5986366    0.80651903  -2.8583133   10.565577   -13.649888\n",
      "   10.679894  ]]\n",
      "\tLast Episode Reward: -1332\n",
      "********** Iteration 401 ************\n",
      "\tLast Episode Reward: -1332\n",
      "********** Iteration 402 ************\n",
      "\tLast Episode Reward: -1495\n",
      "********** Iteration 403 ************\n",
      "\tLast Episode Reward: -1495\n",
      "********** Iteration 404 ************\n",
      "\tLast Episode Reward: -1519\n",
      "********** Iteration 405 ************\n",
      "\tLast Episode Reward: -1519\n",
      "********** Iteration 406 ************\n",
      "\tLast Episode Reward: -1472\n",
      "********** Iteration 407 ************\n",
      "\tLast Episode Reward: -1472\n",
      "********** Iteration 408 ************\n",
      "\tLast Episode Reward: -1255\n",
      "********** Iteration 409 ************\n",
      "\tLast Episode Reward: -1255\n",
      "********** Iteration 410 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 411 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 412 ************\n",
      "\tLast Episode Reward: -1392\n",
      "********** Iteration 413 ************\n",
      "\tLast Episode Reward: -1392\n",
      "********** Iteration 414 ************\n",
      "\tLast Episode Reward: -1398\n",
      "********** Iteration 415 ************\n",
      "\tLast Episode Reward: -1398\n",
      "********** Iteration 416 ************\n",
      "\tLast Episode Reward: -1495\n",
      "********** Iteration 417 ************\n",
      "\tLast Episode Reward: -1495\n",
      "********** Iteration 418 ************\n",
      "\tLast Episode Reward: -1509\n",
      "********** Iteration 419 ************\n",
      "\tLast Episode Reward: -1509\n",
      "********** Iteration 420 ************\n",
      "\tLast Episode Reward: -1630\n",
      "********** Iteration 421 ************\n",
      "\tLast Episode Reward: -1630\n",
      "********** Iteration 422 ************\n",
      "\tLast Episode Reward: -1354\n",
      "********** Iteration 423 ************\n",
      "\tLast Episode Reward: -1354\n",
      "********** Iteration 424 ************\n",
      "\tLast Episode Reward: -1491\n",
      "********** Iteration 425 ************\n",
      "\tLast Episode Reward: -1491\n",
      "********** Iteration 426 ************\n",
      "\tLast Episode Reward: -1410\n",
      "********** Iteration 427 ************\n",
      "\tLast Episode Reward: -1410\n",
      "********** Iteration 428 ************\n",
      "\tLast Episode Reward: -826\n",
      "********** Iteration 429 ************\n",
      "\tLast Episode Reward: -826\n",
      "********** Iteration 430 ************\n",
      "\tLast Episode Reward: -1344\n",
      "********** Iteration 431 ************\n",
      "\tLast Episode Reward: -1344\n",
      "********** Iteration 432 ************\n",
      "\tLast Episode Reward: -1377\n",
      "********** Iteration 433 ************\n",
      "\tLast Episode Reward: -1377\n",
      "********** Iteration 434 ************\n",
      "\tLast Episode Reward: -1432\n",
      "********** Iteration 435 ************\n",
      "\tLast Episode Reward: -1432\n",
      "********** Iteration 436 ************\n",
      "\tLast Episode Reward: -1550\n",
      "********** Iteration 437 ************\n",
      "\tLast Episode Reward: -1550\n",
      "********** Iteration 438 ************\n",
      "\tLast Episode Reward: -1514\n",
      "********** Iteration 439 ************\n",
      "\tLast Episode Reward: -1514\n",
      "********** Iteration 440 ************\n",
      "\tLast Episode Reward: -1633\n",
      "********** Iteration 441 ************\n",
      "\tLast Episode Reward: -1633\n",
      "********** Iteration 442 ************\n",
      "\tLast Episode Reward: -1562\n",
      "********** Iteration 443 ************\n",
      "\tLast Episode Reward: -1562\n",
      "********** Iteration 444 ************\n",
      "\tLast Episode Reward: -1544\n",
      "********** Iteration 445 ************\n",
      "\tLast Episode Reward: -1544\n",
      "********** Iteration 446 ************\n",
      "\tLast Episode Reward: -1525\n",
      "********** Iteration 447 ************\n",
      "\tLast Episode Reward: -1525\n",
      "********** Iteration 448 ************\n",
      "\tLast Episode Reward: -1660\n",
      "********** Iteration 449 ************\n",
      "\tLast Episode Reward: -1660\n",
      "********** Iteration 450 ************\n",
      "\tLast Episode Reward: -1497\n",
      "********** Iteration 451 ************\n",
      "\tLast Episode Reward: -1497\n",
      "********** Iteration 452 ************\n",
      "\tLast Episode Reward: -1509\n",
      "********** Iteration 453 ************\n",
      "\tLast Episode Reward: -1509\n",
      "********** Iteration 454 ************\n",
      "\tLast Episode Reward: -1616\n",
      "********** Iteration 455 ************\n",
      "\tLast Episode Reward: -1616\n",
      "********** Iteration 456 ************\n",
      "\tLast Episode Reward: -1670\n",
      "********** Iteration 457 ************\n",
      "\tLast Episode Reward: -1670\n",
      "********** Iteration 458 ************\n",
      "\tLast Episode Reward: -961\n",
      "********** Iteration 459 ************\n",
      "\tLast Episode Reward: -961\n",
      "********** Iteration 460 ************\n",
      "\tLast Episode Reward: -1444\n",
      "********** Iteration 461 ************\n",
      "\tLast Episode Reward: -1444\n",
      "********** Iteration 462 ************\n",
      "\tLast Episode Reward: -1507\n",
      "********** Iteration 463 ************\n",
      "\tLast Episode Reward: -1507\n",
      "********** Iteration 464 ************\n",
      "\tLast Episode Reward: -1495\n",
      "********** Iteration 465 ************\n",
      "\tLast Episode Reward: -1495\n",
      "********** Iteration 466 ************\n",
      "\tLast Episode Reward: -1489\n",
      "********** Iteration 467 ************\n",
      "\tLast Episode Reward: -1489\n",
      "********** Iteration 468 ************\n",
      "\tLast Episode Reward: -1557\n",
      "********** Iteration 469 ************\n",
      "\tLast Episode Reward: -1557\n",
      "********** Iteration 470 ************\n",
      "\tLast Episode Reward: -1523\n",
      "********** Iteration 471 ************\n",
      "\tLast Episode Reward: -1523\n",
      "********** Iteration 472 ************\n",
      "\tLast Episode Reward: -1523\n",
      "********** Iteration 473 ************\n",
      "\tLast Episode Reward: -1523\n",
      "********** Iteration 474 ************\n",
      "\tLast Episode Reward: -1512\n",
      "********** Iteration 475 ************\n",
      "\tLast Episode Reward: -1512\n",
      "********** Iteration 476 ************\n",
      "\tLast Episode Reward: -1427\n",
      "********** Iteration 477 ************\n",
      "\tLast Episode Reward: -1427\n",
      "********** Iteration 478 ************\n",
      "\tLast Episode Reward: -1501\n",
      "********** Iteration 479 ************\n",
      "\tLast Episode Reward: -1501\n",
      "********** Iteration 480 ************\n",
      "\tLast Episode Reward: -1616\n",
      "********** Iteration 481 ************\n",
      "\tLast Episode Reward: -1616\n",
      "********** Iteration 482 ************\n",
      "\tLast Episode Reward: -1525\n",
      "********** Iteration 483 ************\n",
      "\tLast Episode Reward: -1525\n",
      "********** Iteration 484 ************\n",
      "\tLast Episode Reward: -1443\n",
      "********** Iteration 485 ************\n",
      "\tLast Episode Reward: -1443\n",
      "********** Iteration 486 ************\n",
      "\tLast Episode Reward: -1395\n",
      "********** Iteration 487 ************\n",
      "\tLast Episode Reward: -1395\n",
      "********** Iteration 488 ************\n",
      "\tLast Episode Reward: -1559\n",
      "********** Iteration 489 ************\n",
      "\tLast Episode Reward: -1559\n",
      "********** Iteration 490 ************\n",
      "\tLast Episode Reward: -1372\n",
      "********** Iteration 491 ************\n",
      "\tLast Episode Reward: -1372\n",
      "********** Iteration 492 ************\n",
      "\tLast Episode Reward: -1847\n",
      "********** Iteration 493 ************\n",
      "\tLast Episode Reward: -1847\n",
      "********** Iteration 494 ************\n",
      "\tLast Episode Reward: -1483\n",
      "********** Iteration 495 ************\n",
      "\tLast Episode Reward: -1483\n",
      "********** Iteration 496 ************\n",
      "\tLast Episode Reward: -1587\n",
      "********** Iteration 497 ************\n",
      "\tLast Episode Reward: -1587\n",
      "********** Iteration 498 ************\n",
      "\tLast Episode Reward: -1362\n",
      "********** Iteration 499 ************\n",
      "\tLast Episode Reward: -1362\n",
      "********** Iteration 500 ************\n",
      "[[ 1.05793781e+01  5.21115303e+00  1.19230843e+00 -6.76612186e+00\n",
      "   1.10682573e+01 -3.31985521e+00]\n",
      " [ 2.79626060e+00  4.77947092e+00 -5.62427789e-02  1.37351155e-02\n",
      "   2.39292622e+00  7.45545983e-01]\n",
      " [ 9.97480869e-01  8.87292027e-01  1.38418317e+00 -3.91227722e-01\n",
      "   1.77044535e+00 -3.86653423e-01]\n",
      " [-8.75118065e+00 -6.95491552e-01 -1.22404099e+00  1.60452576e+01\n",
      "  -1.83169727e+01  9.77017498e+00]\n",
      " [ 1.14016094e+01  2.83449602e+00  1.93631744e+00 -1.80518990e+01\n",
      "   2.38651543e+01 -1.22130079e+01]\n",
      " [-4.50020313e+00  9.13228750e-01 -8.34304333e-01  1.02546673e+01\n",
      "  -1.24519873e+01  9.49652004e+00]]\n",
      "\tLast Episode Reward: -1498\n",
      "********** Iteration 501 ************\n",
      "\tLast Episode Reward: -1498\n",
      "********** Iteration 502 ************\n",
      "\tLast Episode Reward: -1295\n",
      "********** Iteration 503 ************\n",
      "\tLast Episode Reward: -1295\n",
      "********** Iteration 504 ************\n",
      "\tLast Episode Reward: -1366\n",
      "********** Iteration 505 ************\n",
      "\tLast Episode Reward: -1366\n",
      "********** Iteration 506 ************\n",
      "\tLast Episode Reward: -1493\n",
      "********** Iteration 507 ************\n",
      "\tLast Episode Reward: -1493\n",
      "********** Iteration 508 ************\n",
      "\tLast Episode Reward: -1642\n",
      "********** Iteration 509 ************\n",
      "\tLast Episode Reward: -1642\n",
      "********** Iteration 510 ************\n",
      "\tLast Episode Reward: -928\n",
      "********** Iteration 511 ************\n",
      "\tLast Episode Reward: -928\n",
      "********** Iteration 512 ************\n",
      "\tLast Episode Reward: -1444\n",
      "********** Iteration 513 ************\n",
      "\tLast Episode Reward: -1444\n",
      "********** Iteration 514 ************\n",
      "\tLast Episode Reward: -1197\n",
      "********** Iteration 515 ************\n",
      "\tLast Episode Reward: -1197\n",
      "********** Iteration 516 ************\n",
      "\tLast Episode Reward: -1619\n",
      "********** Iteration 517 ************\n",
      "\tLast Episode Reward: -1619\n",
      "********** Iteration 518 ************\n",
      "\tLast Episode Reward: -1195\n",
      "********** Iteration 519 ************\n",
      "\tLast Episode Reward: -1195\n",
      "********** Iteration 520 ************\n",
      "\tLast Episode Reward: -1222\n",
      "********** Iteration 521 ************\n",
      "\tLast Episode Reward: -1222\n",
      "********** Iteration 522 ************\n",
      "\tLast Episode Reward: -1339\n",
      "********** Iteration 523 ************\n",
      "\tLast Episode Reward: -1339\n",
      "********** Iteration 524 ************\n",
      "\tLast Episode Reward: -1469\n",
      "********** Iteration 525 ************\n",
      "\tLast Episode Reward: -1469\n",
      "********** Iteration 526 ************\n",
      "\tLast Episode Reward: -1067\n",
      "********** Iteration 527 ************\n",
      "\tLast Episode Reward: -1067\n",
      "********** Iteration 528 ************\n",
      "\tLast Episode Reward: -1403\n",
      "********** Iteration 529 ************\n",
      "\tLast Episode Reward: -1403\n",
      "********** Iteration 530 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 531 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 532 ************\n",
      "\tLast Episode Reward: -1372\n",
      "********** Iteration 533 ************\n",
      "\tLast Episode Reward: -1372\n",
      "********** Iteration 534 ************\n",
      "\tLast Episode Reward: -1405\n",
      "********** Iteration 535 ************\n",
      "\tLast Episode Reward: -1405\n",
      "********** Iteration 536 ************\n",
      "\tLast Episode Reward: -1487\n",
      "********** Iteration 537 ************\n",
      "\tLast Episode Reward: -1487\n",
      "********** Iteration 538 ************\n",
      "\tLast Episode Reward: -1654\n",
      "********** Iteration 539 ************\n",
      "\tLast Episode Reward: -1654\n",
      "********** Iteration 540 ************\n",
      "\tLast Episode Reward: -1493\n",
      "********** Iteration 541 ************\n",
      "\tLast Episode Reward: -1493\n",
      "********** Iteration 542 ************\n",
      "\tLast Episode Reward: -1316\n",
      "********** Iteration 543 ************\n",
      "\tLast Episode Reward: -1316\n",
      "********** Iteration 544 ************\n",
      "\tLast Episode Reward: -1372\n",
      "********** Iteration 545 ************\n",
      "\tLast Episode Reward: -1372\n",
      "********** Iteration 546 ************\n",
      "\tLast Episode Reward: -1345\n",
      "********** Iteration 547 ************\n",
      "\tLast Episode Reward: -1345\n",
      "********** Iteration 548 ************\n",
      "\tLast Episode Reward: -1465\n",
      "********** Iteration 549 ************\n",
      "\tLast Episode Reward: -1465\n",
      "********** Iteration 550 ************\n",
      "\tLast Episode Reward: -1220\n",
      "********** Iteration 551 ************\n",
      "\tLast Episode Reward: -1220\n",
      "********** Iteration 552 ************\n",
      "\tLast Episode Reward: -1503\n",
      "********** Iteration 553 ************\n",
      "\tLast Episode Reward: -1503\n",
      "********** Iteration 554 ************\n",
      "\tLast Episode Reward: -1448\n",
      "********** Iteration 555 ************\n",
      "\tLast Episode Reward: -1448\n",
      "********** Iteration 556 ************\n",
      "\tLast Episode Reward: -1500\n",
      "********** Iteration 557 ************\n",
      "\tLast Episode Reward: -1500\n",
      "********** Iteration 558 ************\n",
      "\tLast Episode Reward: -1641\n",
      "********** Iteration 559 ************\n",
      "\tLast Episode Reward: -1641\n",
      "********** Iteration 560 ************\n",
      "\tLast Episode Reward: -1396\n",
      "********** Iteration 561 ************\n",
      "\tLast Episode Reward: -1396\n",
      "********** Iteration 562 ************\n",
      "\tLast Episode Reward: -1652\n",
      "********** Iteration 563 ************\n",
      "\tLast Episode Reward: -1652\n",
      "********** Iteration 564 ************\n",
      "\tLast Episode Reward: -1505\n",
      "********** Iteration 565 ************\n",
      "\tLast Episode Reward: -1505\n",
      "********** Iteration 566 ************\n",
      "\tLast Episode Reward: -1497\n",
      "********** Iteration 567 ************\n",
      "\tLast Episode Reward: -1497\n",
      "********** Iteration 568 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 569 ************\n",
      "\tLast Episode Reward: -1496\n",
      "********** Iteration 570 ************\n",
      "\tLast Episode Reward: -1081\n",
      "********** Iteration 571 ************\n",
      "\tLast Episode Reward: -1081\n",
      "********** Iteration 572 ************\n",
      "\tLast Episode Reward: -1419\n",
      "********** Iteration 573 ************\n",
      "\tLast Episode Reward: -1419\n",
      "********** Iteration 574 ************\n",
      "\tLast Episode Reward: -1420\n",
      "********** Iteration 575 ************\n",
      "\tLast Episode Reward: -1420\n",
      "********** Iteration 576 ************\n",
      "\tLast Episode Reward: -1649\n",
      "********** Iteration 577 ************\n",
      "\tLast Episode Reward: -1649\n",
      "********** Iteration 578 ************\n",
      "\tLast Episode Reward: -1065\n",
      "********** Iteration 579 ************\n",
      "\tLast Episode Reward: -1065\n",
      "********** Iteration 580 ************\n",
      "\tLast Episode Reward: -1241\n",
      "********** Iteration 581 ************\n",
      "\tLast Episode Reward: -1241\n",
      "********** Iteration 582 ************\n",
      "\tLast Episode Reward: -1453\n",
      "********** Iteration 583 ************\n",
      "\tLast Episode Reward: -1453\n",
      "********** Iteration 584 ************\n",
      "\tLast Episode Reward: -123\n",
      "********** Iteration 585 ************\n",
      "\tLast Episode Reward: -123\n",
      "********** Iteration 586 ************\n",
      "\tLast Episode Reward: -1293\n",
      "********** Iteration 587 ************\n",
      "\tLast Episode Reward: -1293\n",
      "********** Iteration 588 ************\n",
      "\tLast Episode Reward: -1337\n",
      "********** Iteration 589 ************\n",
      "\tLast Episode Reward: -1337\n",
      "********** Iteration 590 ************\n",
      "\tLast Episode Reward: -1605\n",
      "********** Iteration 591 ************\n",
      "\tLast Episode Reward: -1605\n",
      "********** Iteration 592 ************\n",
      "\tLast Episode Reward: -1437\n",
      "********** Iteration 593 ************\n",
      "\tLast Episode Reward: -1437\n",
      "********** Iteration 594 ************\n",
      "\tLast Episode Reward: -932\n",
      "********** Iteration 595 ************\n",
      "\tLast Episode Reward: -932\n",
      "********** Iteration 596 ************\n",
      "\tLast Episode Reward: -1495\n",
      "********** Iteration 597 ************\n",
      "\tLast Episode Reward: -1495\n",
      "********** Iteration 598 ************\n",
      "\tLast Episode Reward: -1556\n",
      "********** Iteration 599 ************\n",
      "\tLast Episode Reward: -1556\n",
      "********** Iteration 600 ************\n",
      "[[ 10.609133     1.5065724    0.64595985  -3.7865214    3.0261269\n",
      "    0.76578903]\n",
      " [  2.615447     2.976212    -0.19497651  -3.0264459    5.058641\n",
      "   -1.768854  ]\n",
      " [  0.97328186  -0.72508055   1.215173     0.68982935  -1.4284034\n",
      "    1.1497173 ]\n",
      " [ -4.781928    -2.0257518    0.50935745   8.797787   -10.806229\n",
      "    4.7132072 ]\n",
      " [  5.706155     3.8512511   -0.5036011  -12.140953    18.523912\n",
      "   -9.021096  ]\n",
      " [ -0.62688255  -1.1522373    0.665957     5.4164915   -9.047201\n",
      "    6.936241  ]]\n",
      "\tLast Episode Reward: -1246\n",
      "********** Iteration 601 ************\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: model/Cholesky = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/P/read)]]\n\nCaused by op 'model/Cholesky', defined at:\n  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-3e759955276b>\", line 3, in <module>\n    agent.build_model()\n  File \"<ipython-input-35-4637c3d1f5c5>\", line 85, in build_model\n    self.P_asym = tf.linalg.transpose(tf.cholesky(self.P))\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_linalg_ops.py\", line 412, in cholesky\n    \"Cholesky\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: model/Cholesky = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/P/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: model/Cholesky = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/P/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3e759955276b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-39047290cbea>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(agent, env, max_timesteps, max_episodes, max_iters, max_seconds, n_transitions_between_updates, n_optim_steps_per_update, n_iters_per_p_update)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# optimize the model from collected data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_optim_steps_per_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miters_so_far\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_iters_per_p_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-4637c3d1f5c5>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         })\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: model/Cholesky = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/P/read)]]\n\nCaused by op 'model/Cholesky', defined at:\n  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-3e759955276b>\", line 3, in <module>\n    agent.build_model()\n  File \"<ipython-input-35-4637c3d1f5c5>\", line 85, in build_model\n    self.P_asym = tf.linalg.transpose(tf.cholesky(self.P))\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_linalg_ops.py\", line 412, in cholesky\n    \"Cholesky\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cholesky decomposition was not successful. The input might not be valid.\n\t [[Node: model/Cholesky = Cholesky[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](model/P/read)]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = klqr(config,sess)\n",
    "agent.build_model()\n",
    "train_agent(agent,env,max_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
