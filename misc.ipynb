{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# add gradient clipping?\n",
    "\n",
    "# TO TEST:\n",
    "# enforcing norm constraint on P\n",
    "# how to do exploration\n",
    "# whether this works at all?\n",
    "\n",
    "class klqr:\n",
    "    # not currently doing value updates at varying rates\n",
    "    # not currently doing double Q learning (what would this look like?)\n",
    "    \n",
    "    def __init__(self,config,sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.x_dim = config['x_dim']\n",
    "        self.z_dim = config['z_dim']\n",
    "        self.a_dim = config['a_dim']\n",
    "        self.lr = config['lr']\n",
    "        self.horizon = config['horizon']\n",
    "        self.gamma = config['discount_rate']\n",
    "\n",
    "        \n",
    "        ou_theta = config['ou_theta']\n",
    "        ou_sigma = config['ou_sigma']\n",
    "        self.config = config\n",
    "        \n",
    "        # Ornstein-Uhlenbeck noise for exploration -- code from Yuke Zhu\n",
    "        self.noise_var = tf.Variable(tf.zeros([self.a_dim,1]))\n",
    "        noise_random = tf.random_normal([self.a_dim,1], stddev=ou_sigma)\n",
    "        self.noise = self.noise_var.assign_sub((ou_theta) * self.noise_var - noise_random)\n",
    "\n",
    "        self.max_riccati_updates = config['max_riccati_updates']\n",
    "        self.train_batch_size = config['train_batch_size']\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size=config['replay_buffer_size'])\n",
    "        \n",
    "        self.experience_count = 0\n",
    "        \n",
    "    def build_model(self):        \n",
    "\n",
    "        with tf.variable_scope('model',reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            self.x_ = tf.placeholder(tf.float32,shape=[None, self.x_dim])\n",
    "            self.xp_ = tf.placeholder(tf.float32,shape=[None, self.x_dim])\n",
    "            self.a_ = tf.placeholder(tf.float32,shape=[None, self.a_dim])\n",
    "            self.r_ = tf.placeholder(tf.float32,shape=[None])\n",
    "\n",
    "            self.z = self.encoder(self.x_)\n",
    "            self.zp = self.encoder(self.xp_)\n",
    "\n",
    "            print('z shape:', self.z.get_shape())\n",
    "\n",
    "            #init R\n",
    "\n",
    "            self.R_asym = tf.get_variable('R_asym',shape=[self.a_dim,self.a_dim])\n",
    "    #         self.R_asym = tf.Variable(np.random.rand(self.a_dim,self.a_dim) - 0.5)\n",
    "\n",
    "            # working with Ra.T Ra so that inner product is norm(Rx) and not norm(R.T x)\n",
    "            self.R = tf.matmul(tf.transpose(self.R_asym),self.R_asym)\n",
    "\n",
    "            #init Q -- shape: z_dim * z_dim\n",
    "            self.Q_asym = tf.get_variable('Q_asym',shape=[self.z_dim,self.z_dim])\n",
    "            self.Q = tf.matmul(tf.transpose(self.Q_asym),self.Q_asym)\n",
    "\n",
    "            #init P -- shape: z_dim * z_dim\n",
    "            self.P = tf.get_variable('P_asym',shape=[self.z_dim,self.z_dim],trainable=False,initializer=tf.initializers.identity)\n",
    "            self.P_asym = tf.transpose(tf.cholesky(self.P)) #this might need to have the transpose removed?\n",
    "\n",
    "            #init B -- shape: z_dim * u_dim\n",
    "            self.B = tf.get_variable('B',shape=[self.z_dim,self.a_dim])\n",
    "    #         self.B = tf.Variable(np.random.rand(self.z_dim,self.u_dim) - 0.5)\n",
    "\n",
    "            #init A -- shape: z_dim * z_dim\n",
    "            self.A = tf.get_variable('A',shape=[self.z_dim,self.z_dim])\n",
    "    #         self.A = tf.Variable(np.random.rand(self.z_dim,self.z_dim) - 0.5)\n",
    "\n",
    "            #define K -- shape: u_dim * z_dim\n",
    "            term1 = tf.matrix_inverse(self.R + tf.matmul(tf.matmul(tf.transpose(self.B),self.Q),self.B))\n",
    "            term2 = tf.matmul(tf.matmul(tf.transpose(self.B),self.P),self.A)\n",
    "            self.K = tf.matmul(term1,term2)\n",
    "            self.policy_action = tf.transpose(tf.matmul(self.K,tf.transpose(self.z)))\n",
    "\n",
    "            #make reward negative to convert to cost\n",
    "            self.bootstrapped_value = -self.r_ + self.gamma*tf.square(tf.norm(tf.transpose(tf.matmul(self.P_asym,tf.transpose(self.zp))),axis=1))\n",
    "\n",
    "            action_cost = tf.square(tf.norm(tf.transpose(tf.matmul(self.R_asym,tf.transpose(self.a_))),axis=1))#can simplify this by taking norm on other axis\n",
    "            state_cost = tf.square(tf.norm(tf.transpose(tf.matmul(self.Q_asym,tf.transpose(self.z))),axis=1)) \n",
    "            self.PABK = tf.matmul(self.P_asym, self.A + tf.matmul(self.B,self.K))\n",
    "            Vzp = tf.square(tf.norm(tf.transpose(tf.matmul(self.PABK,tf.transpose(self.zp))),axis=1))\n",
    "            self.Qsa = action_cost + state_cost + Vzp\n",
    "\n",
    "            self.td_loss = tf.reduce_mean(self.bootstrapped_value - self.Qsa)\n",
    "            #can add regularization via P, dynamics, sparsity, etc\n",
    "            self.loss = self.td_loss \n",
    "            global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
    "    \n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    def update_model(self):        \n",
    "        #this function is mostly taken from Yuke's code\n",
    "        print('updating model')\n",
    "        if self.replay_buffer.count() < self.train_batch_size:\n",
    "            return\n",
    "        \n",
    "        batch           = self.replay_buffer.getBatch(self.train_batch_size)\n",
    "        \n",
    "        states          = np.zeros((self.train_batch_size, self.x_dim))\n",
    "        rewards         = np.zeros((self.train_batch_size))\n",
    "        actions         = np.zeros((self.train_batch_size, self.a_dim))\n",
    "        next_states     = np.zeros((self.train_batch_size, self.x_dim))\n",
    "\n",
    "        for k, (s0, a, r, s1, done) in enumerate(batch):\n",
    "            #currently throwing away done states; should fix this\n",
    "            states[k] = s0\n",
    "            rewards[k] = r\n",
    "            actions[k] = a\n",
    "            next_states[k] = s1\n",
    "            # check terminal state\n",
    "#             if not done:\n",
    "#                 next_states[k] = s1\n",
    "#                 next_state_mask[k] = 1\n",
    "\n",
    "        cost, _ = self.sess.run([self.loss, self.train_op],\n",
    "        {\n",
    "        self.x_:  states,\n",
    "        self.xp_: next_states,\n",
    "        self.a_:  actions,\n",
    "        self.r_:  rewards\n",
    "        })\n",
    "    \n",
    "        #possibly update target via Riccati recursion? or do standard target separation? \n",
    "    \n",
    "    def update_P(self):\n",
    "        print('updating P')\n",
    "        for k in range(self.max_riccati_updates):\n",
    "            #do Riccati backup in tensorflow oh god why\n",
    "            ABK = self.A + tf.matmul(self.B,self.K)\n",
    "            APA = tf.matmul(tf.matmul(tf.transpose(ABK),self.P),ABK) #\n",
    "            self.P = self.Q + tf.matmul(tf.matmul(tf.transpose(self.K),self.R),self.K) + self.gamma*APA\n",
    "        \n",
    "        self.P_asym = tf.transpose(tf.cholesky(self.P))\n",
    "        print(sess.run(self.P))\n",
    "            #TODO add a termination criterion for norm of Riccati update difference?\n",
    "        \n",
    "    def pi(self,x,explore=True):\n",
    "        self.experience_count += 1\n",
    "        x = np.reshape(x,(1,3))\n",
    "        \n",
    "        a,w = self.sess.run([self.policy_action,self.noise], {self.x_: x})\n",
    "        \n",
    "        a = a + w if explore else a\n",
    "        # TODO check the dimension of the output of this\n",
    "        return [a[0,0]]\n",
    "        \n",
    "    def store_experience(self,s,a,r,sp,done):\n",
    "        # currently storing experience for every iteration\n",
    "        self.replay_buffer.add(s, a, r, sp, done)\n",
    "    \n",
    "    def encoder(self,x,name=\"encoder\",batch_norm=False):\n",
    "        layer_sizes = self.config['encoder_layers']\n",
    "        with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "            inp = x\n",
    "            for units in layer_sizes: \n",
    "                inp = tf.layers.dense(inputs=inp, units=units,activation=tf.nn.relu)\n",
    "\n",
    "            z = tf.layers.dense(inputs=inp, units=self.z_dim,activation=None)\n",
    "\n",
    "        if batch_norm:\n",
    "            z = tf.layers.batch_normalization(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "class ReplayBuffer:\n",
    "    # taken from Yuke Zhu's Q learning implementation\n",
    "    \n",
    "    def __init__(self, buffer_size):\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        # random draw N\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, next_action, done):\n",
    "        new_experience = (state, action, reward, next_action, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "          self.buffer.append(new_experience)\n",
    "          self.num_experiences += 1\n",
    "        else:\n",
    "          self.buffer.popleft()\n",
    "          self.buffer.append(new_experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yml','r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulates the agent acting in env, yielding every N steps\n",
    "# (decouples episode reseting mechanics from the training alg)\n",
    "def experience_generator(agent, env, N):\n",
    "    s = env.reset()\n",
    "    n_steps = 0\n",
    "    n_eps = 0\n",
    "    last_cum_rew = 0\n",
    "    cum_rew = 0\n",
    "    while True:\n",
    "        n_steps += 1\n",
    "        a = agent.pi(s)\n",
    "        sp, r, done,_ = env.step(a)\n",
    "        cum_rew += r\n",
    "        if done:\n",
    "            n_eps += 1\n",
    "            last_cum_rew = cum_rew\n",
    "            cum_rew = 0\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            agent.store_experience(s, a, r, sp, done)\n",
    "            s = sp\n",
    "\n",
    "        if n_steps % N == 0:\n",
    "            yield (n_steps, n_eps, last_cum_rew)\n",
    "\n",
    "\n",
    "\n",
    "def train_agent(agent, env,\n",
    "                max_timesteps=0, max_episodes=0, max_iters=0, max_seconds=0, # time constraint\n",
    "                n_transitions_between_updates=100,\n",
    "                n_optim_steps_per_update=100,\n",
    "                n_optim_steps_per_p_update=100,\n",
    "                ):\n",
    "\n",
    "    # run an episode, and feed data to model\n",
    "    episodes_so_far = 0\n",
    "    timesteps_so_far = 0\n",
    "    iters_so_far = 0\n",
    "    tstart = time.time()\n",
    "\n",
    "    assert sum([max_iters>0, max_timesteps>0, max_episodes>0, max_seconds>0])==1, \"Only one time constraint permitted\"\n",
    "\n",
    "    exp_gen = experience_generator(agent, env, n_transitions_between_updates)\n",
    "\n",
    "    while True:\n",
    "        iters_so_far += 1\n",
    "        if max_timesteps and timesteps_so_far >= max_timesteps:\n",
    "            break\n",
    "        elif max_episodes and episodes_so_far >= max_episodes:\n",
    "            break\n",
    "        elif max_iters and iters_so_far >= max_iters:\n",
    "            break\n",
    "        elif max_seconds and time.time() - tstart >= max_seconds:\n",
    "            break\n",
    "\n",
    "        print(\"********** Iteration %i ************\"%iters_so_far)\n",
    "\n",
    "        # gather experience\n",
    "        episodes_so_far, timesteps_so_far, last_cum_rew = exp_gen.__next__()\n",
    "\n",
    "        # optimize the model from collected data:\n",
    "        for i in range(n_optim_steps_per_update):\n",
    "            agent.update_model()\n",
    "\n",
    "            if (i+1) % n_optim_steps_per_p_update == 0:\n",
    "                agent.update_P()\n",
    "\n",
    "        print(\"\\tLast Episode Reward: %d\"%last_cum_rew)\n",
    "        # add other logging stuff here\n",
    "        # add saving checkpoints here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "agent = klqr(config,sess)\n",
    "agent.build_model()\n",
    "train_agent(agent,env,max_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
